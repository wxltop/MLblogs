import{_ as i,r as n,o,c as l,a as e,b as t,d as r,e as s}from"./app-aa9cafec.js";const c="/MLblogs/assets/2023-12-20-14-52-42-image-3ccef6e7.png",d="/MLblogs/assets/2023-12-20-15-25-22-image-6600e890.png",p="/MLblogs/assets/2023-12-21-22-47-46-image-1876a3c8.png",h="/MLblogs/assets/2023-12-21-22-47-37-image-e8cbf812.png",g="/MLblogs/assets/2023-12-20-16-34-42-image-4485032f.png",_="/MLblogs/assets/2023-12-20-20-05-22-image-2fe40532.png",m="/MLblogs/assets/2023-12-20-20-41-30-image-280fcea1.png",L="/MLblogs/assets/2023-12-20-21-14-28-image-0891087f.png",I="/MLblogs/assets/2023-12-20-21-45-17-image-f6b3c1f3.png",u="/MLblogs/assets/2023-12-21-22-41-25-image-5058a25c.png",b={},f={class:"custom-container info"},P=e("svg",{xmlns:"http://www.w3.org/2000/svg","xmlns:xlink":"http://www.w3.org/1999/xlink",viewBox:"0 0 24 24"},[e("g",{fill:"none",stroke:"currentColor","stroke-width":"2","stroke-linecap":"round","stroke-linejoin":"round"},[e("circle",{cx:"12",cy:"12",r:"9"}),e("path",{d:"M12 8h.01"}),e("path",{d:"M11 12h1v4h1"})])],-1),x=e("p",{class:"custom-container-title"},"INFO",-1),M={href:"https://www.bilibili.com/video/BV1Ax4y1d7JM/?spm_id_from=333.337.search-card.all.click&vd_source=29624dbb703a504c9a36c90ccf9558d4",target:"_blank",rel:"noopener noreferrer"},B=s('<h2 id="什么是多模态" tabindex="-1"><a class="header-anchor" href="#什么是多模态" aria-hidden="true">#</a> 什么是多模态？</h2><p>多模态概念的产生是由于存在多种不同模式的信息。这些信息往往来自不同的数据源，经过不同的接收器接收，比如音频、图像、文本等。现实世界中，我们人的身体就有各种各样的接收器，换在计算机领域里呢，虽然有多种传感器，但是面对更多的数据模式还是文本、图像、音频。</p><h2 id="什么是对齐" tabindex="-1"><a class="header-anchor" href="#什么是对齐" aria-hidden="true">#</a> 什么是对齐？</h2><p>假设有对同一只小狗的多个模式的描述，比如文本：dog，图像，声音。我们使用三个单独的编码器去提取信息，因为我们都描述的是同一个对象，所以我们希望这多个模态的向量能尽可能相近。</p><img title="" src="'+c+'" alt="" data-align="center" width="556"><h2 id="表征学习" tabindex="-1"><a class="header-anchor" href="#表征学习" aria-hidden="true">#</a> 表征学习</h2><p>在保留图片信息的前提下，将图像转为向量。</p><h2 id="迁移学习" tabindex="-1"><a class="header-anchor" href="#迁移学习" aria-hidden="true">#</a> 迁移学习</h2><p>在LLM出来之前，我们通常在一个比较大的数据集中训练一个模型，然后面对下游任务的时候，再在一个小数据集中微调模型的参数。</p><p>LLM出来之后，微调一次需要花费很大的成本，所以有人提出zero-short.</p><h2 id="zero-shot" tabindex="-1"><a class="header-anchor" href="#zero-shot" aria-hidden="true">#</a> zero-shot</h2><p>先在大量数据中训练模型，让其获得丰富的知识，在遇到没有见过的样本的时候，能根据之前学到的知识，利用之前接触过的样本的相同点和不同点来推导出该样本的特性，而不用再在包含这些样本的数据集中训练。</p><h2 id="clip" tabindex="-1"><a class="header-anchor" href="#clip" aria-hidden="true">#</a> CLIP</h2><p>打破语言与视觉的界限。它可以在给定图像的情况下，使用自然语言来预测最相关的文本片段，而无需为特定任务进行优化。CLIP的设计类似于GPT-2和GPT-3，具备出色的零射击能力，可以应用于多种多模态任务。</p><ul><li><p>多模态对比语言图像预训练（CLIP）是一种神经网络模型，它通过多模态对比训练来学习图像和文本之间的关联。与传统的单模态预训练模型不同，CLIP能够同时处理图像和文本，从而更好地理解它们之间的语义关系。</p></li><li><p>CLIP的设计类似于GPT-2和GPT-3，是一种自回归语言模型。它通过对比学习来学习图像和文本之间的映射关系。在训练过程中，CLIP会接收一张图像和一个与之相关的文本片段，并学习如何将这两个模态的信息进行关联。通过这种方式，CLIP可以学会将图像与相应的文本片段进行匹配，从而在给定图像的情况下，使用自然语言来预测最相关的文本片段。</p></li><li><p>由于CLIP采用了对比学习的方法，它可以在无需为特定任务进行优化的前提下，表现出色地完成多种多模态任务。这使得CLIP成为了一种通用的多模态预训练模型，可以广泛应用于图像标注、视觉问答、图像生成等领域。</p></li></ul><p>参考：</p>',16),w={href:"https://www.nowcoder.com/discuss/548873731888648192?sourceSSR=search",target:"_blank",rel:"noopener noreferrer"},C=s('<p>OpenAI先在互联网中爬取4亿个文本对，然后使用3w的batch size训练网络的对齐能力。先对文本和图像编码成向量（文本使用BERT，图像使用VIT），向量维度为768。</p><img src="'+d+'" title="" alt="" data-align="center"><p>训练阶段构建一个矩阵label，该矩阵对每一个文本图像对的相似度进行监督。测试阶段，将尽可能多的类别文本拿到一个prompt句式中构建prompt，然后计算和测试输入的图像之间的相似度，然后求最大值对应的那个文本。</p><p>除了做文本和图像的配对，其实还可以做图像-图像配对，文本-文本配对。</p><h3 id="clip与分类网络的区别" tabindex="-1"><a class="header-anchor" href="#clip与分类网络的区别" aria-hidden="true">#</a> CLIP与分类网络的区别？</h3><p>任务上其实都是分类，但是CLIP其实强调的是zero-shot，即对于未见过的样本的分类，这一点是分类网络做不到的。</p><h2 id="dino" tabindex="-1"><a class="header-anchor" href="#dino" aria-hidden="true">#</a> DINO</h2>',7),v={href:"https://arxiv.org/abs/2104.14294",target:"_blank",rel:"noopener noreferrer"},k=s('<img title="" src="'+p+'" alt="" data-align="center" width="233"><img src="'+h+'" title="" alt="" data-align="center"><p>上面两个图是其网络结构和训练过程，使用两个完全一样的网络结构来作为student和teacher。训练的时候对每一张图像做两种数据增强，一种输入student，一种输入teacher。使用交叉熵误差来优化student网络，然后使用EMA方式将student网络参数用于更新teacher网络。</p><p><strong>DINO和CLIP的不同在于，前者用于图-图对齐，CLIP用于图-文对齐</strong>。其次，CLIP的对齐专注于高度抽象的信息，比如同属于兔子，则可能<strong>无法区分兔子更细分的种类</strong>。但是<strong>DINO则可以进行更细粒度的对齐</strong>。其次图像经过DINO之后的特征很鲁棒，具有很明显的物体区域（分割），该特征使用一个简单的KNN分类效果都很好。</p><p>DINO的升级版---DINOv2效果更加，可以完成多项下游任务，如深度估计，分类，关键点匹配，关键区域匹配，图像检索等。</p><h2 id="blip" tabindex="-1"><a class="header-anchor" href="#blip" aria-hidden="true">#</a> BLIP</h2><p>CLIP只能解决对已有的文本和图像匹配，不能生成，所以完成的是自然语言理解任务。而BLIP完成的是自然语言生成任务（当然也能完成图文匹配）。</p><p>BLIP一大贡献在于将自然语言理解和自然语言生成任务进行了融合形成了多模态通用模型。</p><p>如下图所示，左边使用一个transformer的encoder将图像编码成向量（768维），右边其实是一个模块多次使用，颜色相同的表示共享参数，颜色不同的表示不同模块。<code>Bi Self-Att</code>是来自BERT的双向自注意力模块。第一步将Text Encoder得到的文本向量和图像向量计算对比学习损失，让图像和文本对齐；然后第二步将图像特征和文本特征交互一下，计算图像文本匹配损失，来区分正例和负例，正例好解决，就是对齐的图像文本对，但是负例需要有一定的难度，不然难以训练，所以负例来自第一步中匹配失败的图像文本对；第三步执行和GPT一样，将文本预测出来。</p><img src="'+g+'" title="" alt="" data-align="center"><p>还有一大贡献是数据筛选。下图左上角，红色表示不是完全对齐的图像文本对（弱监督），绿色表示完全对齐的图像文本对（强监督）。先使用所有数据训练一段时间，可以得到两个模型：图文匹配模型，文字生成模型。然后使用强监督的样本对，再次训练这两个模型，让其更加准确。然后将可能不是很匹配的样本传给图文匹配模型，如果判断出来该图像文本不匹配则丢弃，若匹配，则加入有效数据，供下一轮使用。其次将网上爬取的图像传入文字生成模型中，让其生成对应的文字，然后这生成的图文对还不一定是完全对齐的，所以也会传入到数据清洗部分去清洗。</p><img src="'+_+'" title="" alt="" data-align="center"><h2 id="blip2" tabindex="-1"><a class="header-anchor" href="#blip2" aria-hidden="true">#</a> BLIP2</h2><p>BLIP的问题是，需要重新训练。BLIP2冻结图像的编码器和文本解码器，训练中间部分，通过将图像向量转换成与文本对齐的形式。</p><img title="" src="'+m+'" alt="" data-align="center" width="462"><p>第一阶段：下图中左边的Q-former模块使用几个可学习embeddings来提取图像中与文字最相关的视觉表征。右边是为了让模型具有生成文字的能力，所以要对预测的句子进行mask。最终左边的绿色FFN输出的特征将被用于LLM来解码。</p><img src="'+L+'" title="" alt="" data-align="center"><p>第二阶段，在从第一阶段得到向量之后，将该向量传入LLM中去生成文本，这个向量就起到了prompt的作用，这是一个融合了图像上下文和文本上下文的prompt。这个图有上下两种结构，其实描述的是两种不同结构的LLM。所以BLIP2的功能就是生成有效的向量，然后利用该向量生成文本。</p><img src="'+I+'" title="" alt="" data-align="center"><p>BLIP2的优势是需要微调的参数非常少。blip2可以很好的理解图像，但是可能由于结构（VIT可能对图像细节处理不到位）原因，对于部分细节把握不到位。</p><p>量化、蒸馏、高性能运算等可以加速大模型的推理。</p><h2 id="visualglm" tabindex="-1"><a class="header-anchor" href="#visualglm" aria-hidden="true">#</a> VisualGLM</h2>',22),N={href:"https://github.com/THUDM/VisualGLM-6B?tab=readme-ov-file#visualglm-6b",target:"_blank",rel:"noopener noreferrer"},V=e("p",null,"这篇论文结构和BLIP-V2一样，只是在VIT和LLM中加了Lora来微调，这一点不同于BLIP-V2。其次还有不同的是支持中文文本输入。",-1),T=e("img",{title:"",src:u,alt:"","data-align":"center",width:"557"},null,-1);function D(G,O){const a=n("ExternalLinkIcon");return o(),l("div",null,[e("div",f,[P,x,e("p",null,[e("a",M,[t("2023最火的多模态大模型到底是什么？前科大讯飞大佬强力讲解CLIP BLIP BLIP2三大模型，创新点十足的研究方向！_哔哩哔哩_bilibili"),r(a)])])]),B,e("p",null,[e("a",w,[t("多模态对比语言图像预训练CLIP：打破语言与视觉的界限_牛客网"),r(a)])]),C,e("p",null,[e("a",v,[t("[2104.14294] Emerging Properties in Self-Supervised Vision Transformers (arxiv.org)"),r(a)])]),k,e("p",null,[e("a",N,[t("THUDM/VisualGLM-6B: Chinese and English multimodal conversational language model | 多模态中英双语对话语言模型 (github.com)"),r(a)])]),V,T])}const z=i(b,[["render",D],["__file","CLIP、DINO、BLIP、BLIPv2、VisualGLM.html.vue"]]);export{z as default};
