import{_ as r,r as s,o as a,c as l,a as o,b as t,d as e,e as i}from"./app-aa9cafec.js";const g={},c={class:"custom-container info"},p=o("svg",{xmlns:"http://www.w3.org/2000/svg","xmlns:xlink":"http://www.w3.org/1999/xlink",viewBox:"0 0 24 24"},[o("g",{fill:"none",stroke:"currentColor","stroke-width":"2","stroke-linecap":"round","stroke-linejoin":"round"},[o("circle",{cx:"12",cy:"12",r:"9"}),o("path",{d:"M12 8h.01"}),o("path",{d:"M11 12h1v4h1"})])],-1),h=o("p",{class:"custom-container-title"},"INFO",-1),d=o("p",null,"本文记录DataWhale开源的教程《动手学大模型应用开发》的学习内容。",-1),u={href:"https://github.com/datawhalechina/llm-universe",target:"_blank",rel:"noopener noreferrer"},L={href:"https://datawhalechina.github.io/llm-universe/#/C1/2.%20%E2%BC%A4%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%83%BD%E2%BC%92%E5%92%8C%E7%89%B9%E7%82%B9",target:"_blank",rel:"noopener noreferrer"},M=i('<h2 id="一、大模型简介" tabindex="-1"><a class="header-anchor" href="#一、大模型简介" aria-hidden="true">#</a> 一、大模型简介</h2><h3 id="大模型的能力、特点" tabindex="-1"><a class="header-anchor" href="#大模型的能力、特点" aria-hidden="true">#</a> 大模型的能力、特点</h3><p>大模型的能力：</p><blockquote><p><strong>涌现能力</strong>：涌现能力指的是一种令人惊讶的能力，它在小型模型中不明显，但在大型模型中显著出现。涌现能力的显现就像是模型性能随着规模增大而迅速提升，类似量变引起质变。</p><p>涌现能力可以定义为与某些复杂任务相关的能力，但我们更关注的是它们具备的通用能力，三个典型的LLM涌现能力：</p><ol><li><p><strong>上下文学习</strong>：上下文学习能力是由 GPT-3 首次引入的。这种能力允许语言模型在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而无需额外的训练或参数更新。</p></li><li><p><strong>指令遵循</strong>：通过使用自然语言描述的多任务数据进行微调，也就是所谓的指令微调，LLM 被证明在同样使用指令形式化描述的未见过的任务上表现良好。这意味着LLM能够根据任务指令执行任务，而无需事先见过具体示例，这展示了其强大的泛化能力。</p></li><li><p><strong>逐步推理</strong>：小型语言模型通常难以解决涉及多个推理步骤的复杂任务，例如数学问题。然而，LLM通过采用&quot;思维链&quot;推理策略，可以利用包含中间推理步骤的提示机制来解决这些任务，从而得出最终答案。据推测，这种能力可能是通过对代码的训练获得的。</p></li></ol></blockquote><blockquote><p><strong>作为基座模型支持多元应用的能力</strong>：</p><p>借助于海量无标注数据的训练，获得可以适用于大量下游任务的大模型（单模态或者多模态）。这样，多个应用可以只依赖于一个或少数几个大模型进行统一建设。</p></blockquote><blockquote><p><strong>支持对话作为统一入口的能力</strong></p></blockquote><p>大模型的特点：</p><blockquote><ul><li><p><strong>巨大的规模：</strong> LLM通常具有巨大的参数规模，可以达到数十亿甚至数千亿个参数。</p></li><li><p><strong>预训练和微调：</strong> LLM采用了预训练和微调的学习方法。它们首先在大规模文本数据上进行预训练（无标签数据），学会了通用的语言表示和知识，然后通过微调（有标签数据）适应特定任务，从而在各种NLP任务中表现出色。</p></li><li><p><strong>上下文感知：</strong> LLM在处理文本时具有强大的上下文感知能力，能够理解和生成依赖于前文的文本内容。这使得它们在对话、文章生成和情境理解方面表现出色。</p></li><li><p><strong>多语言支持：</strong> LLM可以用于多种语言，使得跨文化和跨语言的应用变得更加容易。</p></li><li><p><strong>多模态支持：</strong> 一些LLM已经扩展到支持多模态数据，包括文本、图像和声音。</p></li><li><p><strong>涌现能力：</strong> LLM表现出令人惊讶的涌现能力，即在大规模模型中出现但在小型模型中不明显的性能提升。这使得它们能够处理更复杂的任务和问题。</p></li><li><p><strong>多领域应用：</strong> LLM已经被广泛应用于文本生成、自动翻译、信息检索、摘要生成、聊天机器人、虚拟助手等多个领域。</p></li><li><p><strong>伦理和风险问题：</strong> 尽管LLM具有出色的能力，但它们也引发了伦理和风险问题，包括生成有害内容、隐私问题、认知偏差等。</p></li></ul></blockquote><h3 id="常见大模型" tabindex="-1"><a class="header-anchor" href="#常见大模型" aria-hidden="true">#</a> 常见大模型</h3><p><strong>GPT系列</strong>（闭源）：</p><blockquote><p>GPT 模型的基本原则是<strong>通过语言建模将世界知识压缩到仅解码器的 Transformer 模型中</strong>，这样它就可以恢复(或记忆)世界知识的语义，并充当通用任务求解器。它能够成功的两个关键点：</p><ul><li>训练能够准确预测下一个单词的仅解码器的 Transformer 语言模型</li><li>扩展语言模型的大小。</li></ul><p>GPT3.5 拥有 1750亿 个参数，而 GPT4 的参数量官方并没有公布，但有相关人员猜测，GPT-4 在 120 层中总共包含了 1.8 万亿参数，也就是说，GPT-4 的规模是 GPT-3 的 10 倍以上。</p></blockquote><p><strong>Claude系列</strong>（闭源）：</p><blockquote><p>Claude 系列模型是由 OpenAI 离职人员创建的 <strong>Anthropic</strong> 公司开发的闭源语言大模型，可以完成摘要总结、搜索、协助创作、问答、编码等任务。该系列模型通过无监督预训练、基于人类反馈的强化学习和 Constitutional AI 技术（包含监督训练和强化学习）进行训练，旨在改进模型的有用性、诚实性和无害性。</p></blockquote><p><strong>PaML系列</strong>（闭源）：</p><blockquote><p><strong>PaLM 系列</strong>语言大模型由 <strong>Google</strong> 开发。 基于 Google 提出的 Pathways 机器学习系统搭建。PaLM2 的突破：</p><ol><li>最优的缩放比例（训练数据大小/模型参数量），通过 compute-optimal scaling 的研究，可以得知数据大小与模型大小同样重要。根据谷歌的研究，数据和模型大小大致按照 1：1 的比例缩放，可以达到最佳性能。（过去常认为，模型参数量的大小大致为数据集 3 倍更佳）</li></ol></blockquote><p><strong>文心一言</strong>（闭源）：</p><blockquote><p>文心大模型参数量非常大，达到了 2600 亿。文心大模型包括 NLP 大模型、CV 大模型、跨模态大模型、生物计算大模型、行业大模型，其中 NLP 大模型主要为 ERNIE 系列模型。文心一言一方面采用有监督精调、人类反馈的强化学习、提示等技术，还具备知识增强、检索增强和对话增强等关键技术。</p></blockquote><p><strong>星火大模型</strong>（闭源）：</p><blockquote><p><code>讯飞星火 V2.0</code> 升级发布的多模态能力，已实现<strong>图像描述、图像理解、图像推理、识图创作、文图生成、虚拟人合成</strong>。星火大模型包含超过<strong>1700 亿个参数</strong>，来源于数十亿的语言数据集。</p></blockquote><p><strong>LLaMA系列</strong>（开源）：</p><blockquote><p><strong>LLaMA 系列模型</strong>是 Meta 开源的一组参数规模 <strong>从 7B 到 70B</strong> 的基础语言模型，它们都是在数万亿个字符上训练的，展示了如何<strong>仅使用公开可用的数据集来训练最先进的模型</strong>，而不需要依赖专有或不可访问的数据集。这些数据集包括 Common Crawl、Wikipedia、OpenWebText2、RealNews、Books 等。LLaMA 模型使用了<strong>大规模的数据过滤和清洗技术</strong>，以提高数据质量和多样性，减少噪声和偏见。LLaMA 模型还使用了高效的<strong>数据并行</strong>和<strong>流水线并行</strong>技术，以加速模型的训练和扩展。特别地，LLaMA 13B 在 CommonsenseQA 等 9 个基准测试中超过了 GPT-3 (175B)，而 <strong>LLaMA 65B 与最优秀的模型 Chinchilla-70B 和 PaLM-540B 相媲美</strong>。LLaMA 通过使用更少的字符来达到最佳性能，从而在各种推理预算下具有优势。</p><p>与 GPT 系列相同，LLaMA 模型也采用了 <strong>decoder-only</strong> 架构，但同时结合了一些前人工作的改进，例如：</p><ul><li><code>Pre-normalization</code>，为了提高训练稳定性，LLaMA 对每个 Transformer子层的输入进行了 RMSNorm 归一化，这种归一化方法可以避免梯度爆炸和消失的问题，提高模型的收敛速度和性能；</li><li><code>SwiGLU 激活函数</code>，将 ReLU 非线性替换为 SwiGLU 激活函数，增加网络的表达能力和非线性，同时减少参数量和计算量；</li><li><code>RoPE 位置编码</code>，模型的输入不再使用位置编码，而是在网络的每一层添加了位置编码，RoPE 位置编码可以有效地捕捉输入序列中的相对位置信息，并且具有更好的泛化能力。</li></ul></blockquote><p><strong>GLM系列</strong>（开源）：</p><blockquote><p><strong>GLM 系列模型是清华大学和智谱 AI 等合作研发的开源语言大模型</strong>。<code>ChatGLM</code> 是基于 GLM 结构开发的具有 <strong>62 亿参数量</strong>的语言大模型，<strong>支持 2048 的上下文长度</strong>。其使用了包含 1 万亿字符的中英文语料进行训练，能够<strong>支持中文和英文两种语言的任务</strong>。通过监督微调、反馈自助、人类反馈强化学习等多种训练技术。通过 <strong>INT4 量化</strong> 和 <strong>P-Tuning v2</strong> 等高效微调的算法，<strong>ChatGLM 能够在 7G 显存的条件下进行微调</strong>。</p><p>在 ChatGLM 的基础上，2023 年 6 月发布的 <code>ChatGLM 2</code> 使用了包含 <strong>1.4 万亿字符的中英预料进行预训练，并使用人类偏好的数据对模型进行对齐训练</strong>。</p><ul><li>通过 <strong>FlashAttention 技术，ChatGLM 2 能够处理更长的长下文，支持的长下文长度达到了 3.2 万字符</strong></li><li>通过 <strong>Multi-Query Attention 技术，ChatGLM 2 能够进一步地提升推理速度，减小对显卡的显存占用</strong>。</li></ul><p><code>ChatGLM3-6B</code> 是 ChatGLM3 系列中的开源模型，特性如下：</p><ul><li>更强大的基础模型： ChatGLM3-6B 的基础模型 <code>ChatGLM3-6B-Base</code> 采用了更多样的训练数据、更充分的训练步数和更合理的训练策略。在语义、数学、推理、代码、知识等不同角度的数据集上测评显示，ChatGLM3-6B-Base 具有在 10B 以下的基础模型中最强的性能。</li><li>更完整的功能支持： ChatGLM3-6B 采用了全新设计的 Prompt 格式，除正常的多轮对话外。同时原生支持工具调用（Function Call）、代码执行（Code Interpreter）和 Agent 任务等复杂场景。</li><li>更全面的开源序列： 除了对话模型 ChatGLM3-6B 外，还开源了基础模型 ChatGLM3-6B-Base、<code>长文本对话模型 ChatGLM3-6B-32K</code>。以上所有权重对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。</li></ul><p>此外，还开源了 <code>多模态 CogVLM-17B</code> 、以及 <code>智能体 AgentLM</code> ，具体来说：</p><ul><li>在对话模型上，对标 ChatGPT 的是 ChatGLM</li><li>在文生图方面，对标 DALL.E 的是 CogView</li><li>代码生成上，与 Codex 相对的是 CodeGeeX</li><li>搜索增强上，与 WebGPT 相对的是 WebGLM</li><li>在多模态、图文理解领域，与 GPT-4V 对标的有 ChatGLM3</li></ul></blockquote><p><strong>通义千问</strong>（开源）：</p><blockquote><p><strong>通义千问由阿里巴巴基于“通义”大模型研发</strong>。开源包括基础模型Qwen，即 <code>Qwen-7B</code> 和 <code>Qwen-14B</code> ，以及对话模型 <code>Qwen-Chat</code> ，即 Qwen-7B-Chat 和 Qwen-14B-Chat。</p></blockquote><p><strong>Baichuan系列</strong>（开源）：</p><blockquote><p><strong>Baichuan</strong> 是由<strong>百川智能</strong>开发的<strong>开源可商用</strong>的语言大模型，其基于<strong>Transformer 解码器架构</strong>。</p><p><code>Baichuan-7B</code> 是在大约 1.2 万亿字符上训练的 <strong>70 亿参数</strong>模型，支持<strong>中英双语，最大 4096 的上下文窗口长度</strong>。</p><p><code>Baichuan-13B</code> 在 Baichuan-7B 的基础上进一步扩大参数量到 <strong>130 亿</strong>，并且在高质量的语料上训练了 1.4 万亿 字符，超过 LLaMA-13B 40%，是当前开源 13B 尺寸下训练数据量最多的模型。其支持中英双语，使用 ALiBi 位置编码，最大 4096 的上下文窗口长度，使用 rotary-embedding，是现阶段被大多数模型采用的位置编码方案，具有很好的外推性。百川同时开源了<code>预训练</code>和<code>对齐</code>模型，<strong>预训练模型是面向开发者的“基座”，而对齐模型则面向广大需要对话功能的普通用户</strong>。除了原始权重，为实现更高效的推理，百川开源了 INT8 和 INT4 的量化版本，相对非量化版本在几乎没有效果损失的情况下大大降低了部署的机器资源需求。</p><p>Baichuan 2 是百川智能推出的新一代开源大语言模型，<code>Baichuan2-7B</code> 和 <code>Baichuan2-13B</code>，均基于 2.6 万亿 Tokens 的高质量语料训练。Baichuan 2 在多个权威的中文、英文和多语言的通用、领域 benchmark 上取得同尺寸最佳的效果。本次发布包含有 7B、13B 的 Base 和 Chat 版本，并提供了 Chat 版本的 4bits 量化。</p><p>2023 年 10 月 30 日，百川智能发布 <code>Baichuan2-192K</code> 大模型，上下文窗口长度高达 192 K ，<strong>发布时</strong>是全球最长的上下文窗口。Baichuan2-192K 能够一次处理约 35 万个汉字，是目前支持长上下文窗口最优秀大模型 Claude2（支持 100 K上下文窗口，实测约 8 万字）的 4.4 倍。</p></blockquote><h3 id="什么是langchain" tabindex="-1"><a class="header-anchor" href="#什么是langchain" aria-hidden="true">#</a> 什么是LangChain</h3><p><strong>LangChain 框架是一个开源工具，充分利用了大型语言模型的强大能力，以便开发各种下游应用。它的目标是为各种大型语言模型应用提供通用接口，从而简化应用程序的开发流程</strong>。它能够让语言模型与其他数据来源连接，并且允许语言模型与其所处的环境进行互动。其可以将 LLM 模型（对话模型、embedding模型等）、向量数据库、交互层 Prompt、外部知识、外部代理工具整合到一起，进而可以自由构建 LLM 应用。 LangChain 主要由以下 6 个核心模块组成:</p><ul><li><strong>模型输入/输出（Model I/O）</strong>：与语言模型交互的接口</li><li><strong>数据连接（Data connection）</strong>：与特定应用程序的数据进行交互的接口</li><li><strong>链（Chains）</strong>：将组件组合实现端到端应用。</li><li><strong>记忆（Memory）</strong>：用于链的多次运行之间持久化应用程序状态；</li><li><strong>代理（Agents）</strong>：扩展模型的推理能力。用于复杂的应用的调用序列；</li><li><strong>回调（Callbacks）</strong>：扩展模型的推理能力。用于复杂的应用的调用序列；</li></ul>',30);function B(C,b){const n=s("ExternalLinkIcon");return a(),l("div",null,[o("div",c,[p,h,d,o("p",null,[t("项目地址："),o("a",u,[t("GitHub - datawhalechina/llm-universe: 本项目是一个面向小白开发者的大模型应用开发教程，在线阅读地址：https://datawhalechina.github.io/llm-universe/"),e(n)])]),o("p",null,[t("文档链接："),o("a",L,[t("动手学大模型应用开发"),e(n)])])]),M])}const G=r(g,[["render",B],["__file","dongshouxuedamoxingyingyongkaifa.html.vue"]]);export{G as default};
