import{_ as r,r as a,o as i,c as l,a as e,b as t,d as o,e as c}from"./app-aa9cafec.js";const s={},p=c('<h2 id="onnx和onnx-runtime" tabindex="-1"><a class="header-anchor" href="#onnx和onnx-runtime" aria-hidden="true">#</a> ONNX和ONNX Runtime</h2><p><strong>问题：</strong></p><ul><li><p>优化用于推理（或模型评分）的深度学习模型非常困难</p></li><li><p>想要在不同类型的平台（云/Edge、CPU/GPU 等）上获得最佳性能</p></li><li><p>多种框架，会极大增加复杂性</p></li><li><p>优化框架和硬件的所有不同组合非常耗时</p></li></ul><p>ONNX可以将模型转化为用在任意平台和其他多种框架的格式。</p><p><strong>ONNX Runtime:</strong></p><p>ONNX 模型可与ONNX Runtime 配合使用。ONNX Runtime是机器学习模型的通用跨平台加速器，与PyTorch 、TensorFlow 、TFLite、scikit-learn 等框架兼容。 ONNX 运行时（Runtime）利用特定硬件功能优化ONNX 模型的执行。通过这种优化，模型可以在各种硬件平台（包括 CPU、GPU 和专用加速器）上高效、高性能地运行。</p>',6),u={href:"https://learn.microsoft.com/zh-cn/azure/machine-learning/concept-onnx?view=azureml-api-2#install-and-use-onnx-runtime-with-python",target:"_blank",rel:"noopener noreferrer"},N={href:"https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html",target:"_blank",rel:"noopener noreferrer"};function h(d,m){const n=a("ExternalLinkIcon");return i(),l("div",null,[p,e("p",null,[e("a",u,[t("安装 ONNX 运行时并与 Python 配合使用"),o(n)])]),e("p",null,[e("a",N,[t("(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime — PyTorch Tutorials 2.2.0+cu121 documentation"),o(n)])])])}const x=r(s,[["render",h],["__file","deploy.html.vue"]]);export{x as default};
