import{_ as n}from"./2024-03-19-10-25-41-image-a8040aff.js";import{_ as i,r as m,o as p,c as r,a as s,b as a,d as l,e}from"./app-aa9cafec.js";const c="/MLblogs/assets/2024-03-18-21-13-42-image-2fbb6ba2.png",o="/MLblogs/assets/2024-03-18-20-24-59-image-ba85e027.png",h="/MLblogs/assets/2024-03-19-10-29-55-image-1ac344d8.png",g="/MLblogs/assets/2024-03-19-10-38-21-image-13059940.png",d="/MLblogs/assets/2024-03-19-10-43-17-image-506d2e61.png",u="/MLblogs/assets/2024-03-18-20-44-15-image-860cc412.png",v="/MLblogs/assets/2024-03-18-20-45-06-image-335fd22b.png",y="/MLblogs/assets/2024-03-18-20-46-50-image-64e4740a.png",_={},b={class:"custom-container info"},f=s("svg",{xmlns:"http://www.w3.org/2000/svg","xmlns:xlink":"http://www.w3.org/1999/xlink",viewBox:"0 0 24 24"},[s("g",{fill:"none",stroke:"currentColor","stroke-width":"2","stroke-linecap":"round","stroke-linejoin":"round"},[s("circle",{cx:"12",cy:"12",r:"9"}),s("path",{d:"M12 8h.01"}),s("path",{d:"M11 12h1v4h1"})])],-1),w=s("p",{class:"custom-container-title"},"INFO",-1),x={href:"https://www.bilibili.com/video/BV1MS4y147js/?p=5&spm_id_from=pageDriver&vd_source=29624dbb703a504c9a36c90ccf9558d4",target:"_blank",rel:"noopener noreferrer"},k={href:"https://www.bilibili.com/video/BV1Y3411g7Jq/?spm_id_from=333.337.search-card.all.click&vd_source=29624dbb703a504c9a36c90ccf9558d4",target:"_blank",rel:"noopener noreferrer"},z={href:"https://zhuanlan.zhihu.com/p/27234078",target:"_blank",rel:"noopener noreferrer"},M=e('<h2 id="介绍" tabindex="-1"><a class="header-anchor" href="#介绍" aria-hidden="true">#</a> 介绍</h2><p>word2vec用于将单词转为向量表示。有两种模型：CBOW和Skip-gram，前者利用单词的上下文来预测该单词（完形填空）；后者利用单词来预测其上下文。</p><img src="'+c+'" title="" alt="" data-align="center"><h2 id="样本构造" tabindex="-1"><a class="header-anchor" href="#样本构造" aria-hidden="true">#</a> 样本构造</h2><p>一个简单的训练方式是，将一个单词送入网络，输出上下文单词的概率，<strong>如果语料库很大，最后一层每一次预测都要使用一个softmax来预测每一个单词的概率</strong>，相当耗时。</p><p>为了减轻上述负担，一个初始方案是：对于一段文本，用滑动窗口的方式，改变网络的输入单词以及网络需要输出的单词。输入两个单词，看他们是不是对应的前后单词，来预测它们是前后单词的概率，这样很容易利用网上的文本来构造训练样本。但是这样的话，<strong>所有的样本丢入网络之后输出都是1</strong>，这样无法训练。所以需要构造负样本，负样本可以人为来构造，一般三个样本中一个正样本就可以了。</p><p>这样一个word2vec模型的优化目标就从一个多分类问题退化成了一个近似二分类问题。</p><img src="'+o+'" title="" alt="" data-align="center"><p>假设一句话是：“The quick brown fox jumps over the lazy dog.”，窗口大小为5（意思是以中间单词作为输入，预测左边两个、右边两个单词，若某一边不够两个，比如只有一个，就预测一个）。下图实线框是窗口，蓝色表示输入单词，需要预测窗口内其他单词。</p><img src="'+n+'" title="" alt="" data-align="center"><h2 id="skip-gram结构" tabindex="-1"><a class="header-anchor" href="#skip-gram结构" aria-hidden="true">#</a> Skip-gram结构</h2><p>假设词库中有10,000个不同的单词，英文文本无法直接输入神经网络，所以将一个单词表示为10000维的one-hot向量，网络的隐藏层没有激活函数，但是输出神经元需要执行softmax。训练的时候，输入是一个one-hot向量，输出标签也是一个one-hot标签向量。</p><p>输入到隐藏层是10000x300的权重，隐藏层到输出层是300x10000的权重，训练好之后，前者作为look up table，后者丢弃。</p><img src="'+h+'" title="" alt="" data-align="center"><p>在上图中，假设我们正在学习300个特征的词向量。因此，隐藏层将由一个权重矩阵表示，该权重矩阵有10,000行(代表词汇表中的每个单词)和300列(代表每个隐藏神经元)。所以所有这些的最终<strong>目标就是学习这个隐藏层的权重矩阵——推理阶段输出层会扔掉</strong>。将隐藏层的权重作为查找表，希望获得一个单词的向量时，只需查找该查找表即可。</p><img src="'+g+'" title="" alt="" data-align="center"><p>在隐藏层到输出层也有一个300x10000的权重矩阵，计算输入单词和所有单词的关联度，下图表示输入单词&#39;ants&#39;，计算和&#39;car&#39;的关联度。</p><img src="'+d+'" title="" alt="" data-align="center"><h2 id="为何skip-gram有效-直观感受" tabindex="-1"><a class="header-anchor" href="#为何skip-gram有效-直观感受" aria-hidden="true">#</a> 为何skip-gram有效？直观感受</h2><p>首先明确word2vec的任务：学习向量表。并不需要给定一个单词，准确预测下一个但是是谁，所以在训练过程中，设置一个窗口大小，计算窗口内一个单词和其他所有单词的关联度，这就蕴含了上下文关系。</p><p>如果两个不同的单词具有非常相似的“上下文”(也就是说，它们周围可能出现的单词)，那么模型需要为这两个单词输出非常相似的结果。所以，如果两个单词有相似的上下文，那么网络就能为这两个单词学习相似的单词向量。</p><h2 id="目标函数" tabindex="-1"><a class="header-anchor" href="#目标函数" aria-hidden="true">#</a> 目标函数</h2>',22),L=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mfrac",null,[s("mn",null,"1"),s("mi",null,"T")]),s("munderover",null,[s("mo",null,"∑"),s("mrow",null,[s("mi",null,"t"),s("mo",null,"="),s("mn",null,"1")]),s("mi",null,"T")]),s("munder",null,[s("mo",null,"∑"),s("mrow",null,[s("mo",null,"−"),s("mi",null,"c"),s("mo",null,"≤"),s("mi",null,"i"),s("mo",null,"≤"),s("mi",null,"c"),s("mo",{separator:"true"},","),s("mi",null,"i"),s("mo",{mathvariant:"normal"},"≠"),s("mn",null,"0")])]),s("mi",null,"log"),s("mo",null,"⁡"),s("mrow",null,[s("mi",null,"p"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"w"),s("mrow",null,[s("mi",null,"t"),s("mo",null,"+"),s("mi",null,"i")])]),s("mi",{mathvariant:"normal"},"∣"),s("msub",null,[s("mi",null,"w"),s("mi",null,"t")]),s("mo",{stretchy:"false"},")")])]),s("annotation",{encoding:"application/x-tex"}," \\frac{1}{T}\\sum_{t=1}^{T}\\sum_{-c\\leq i\\leq c, i\\neq 0}\\log{p(w_{t+i}|w_t)} ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"3.2666em","vertical-align":"-1.4382em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.3214em"}},[s("span",{style:{top:"-2.314em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"T")])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.677em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.686em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mop op-limits"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.8283em"}},[s("span",{style:{top:"-1.8829em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"t"),s("span",{class:"mrel mtight"},"="),s("span",{class:"mord mtight"},"1")])])]),s("span",{style:{top:"-3.05em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",null,[s("span",{class:"mop op-symbol large-op"},"∑")])]),s("span",{style:{top:"-4.3em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.13889em"}},"T")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.2671em"}},[s("span")])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mop op-limits"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.05em"}},[s("span",{style:{top:"-1.8479em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mathnormal mtight"},"c"),s("span",{class:"mrel mtight"},"≤"),s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mrel mtight"},"≤"),s("span",{class:"mord mathnormal mtight"},"c"),s("span",{class:"mpunct mtight"},","),s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mrel mtight"},[s("span",{class:"mrel mtight"},[s("span",{class:"mord vbox mtight"},[s("span",{class:"thinbox mtight"},[s("span",{class:"rlap mtight"},[s("span",{class:"strut",style:{height:"0.8889em","vertical-align":"-0.1944em"}}),s("span",{class:"inner"},[s("span",{class:"mord mtight"},[s("span",{class:"mrel mtight"},"")])]),s("span",{class:"fix"})])])])]),s("span",{class:"mrel mtight"},"=")]),s("span",{class:"mord mtight"},"0")])])]),s("span",{style:{top:"-3.05em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",null,[s("span",{class:"mop op-symbol large-op"},"∑")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.4382em"}},[s("span")])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mop"},[a("lo"),s("span",{style:{"margin-right":"0.01389em"}},"g")]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.02691em"}},"w"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0269em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"t"),s("span",{class:"mbin mtight"},"+"),s("span",{class:"mord mathnormal mtight"},"i")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2083em"}},[s("span")])])])])]),s("span",{class:"mord"},"∣"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.02691em"}},"w"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0269em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")")])])])])])],-1),V=s("p",null,[a("其中[-c, c]是样本的滑动窗口范围大小。T是词袋大小，"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"w"),s("mi",null,"t")])]),s("annotation",{encoding:"application/x-tex"},"w_t")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.02691em"}},"w"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0269em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("表示的单词。")],-1),W=e('<h2 id="流程" tabindex="-1"><a class="header-anchor" href="#流程" aria-hidden="true">#</a> 流程</h2><p>初始化词向量矩阵：</p><img src="'+u+'" title="" alt="" data-align="center"><p>假设输入的是dataset里的前三列，那么需要拿出四个单词对应的词向量：</p><img src="'+v+'" title="" alt="" data-align="center"><p>通过神经网络反向传播更新，不仅会更新参数矩阵，还会更新输入数据（词向量矩阵）：</p><img src="'+y+'" title="" alt="" data-align="center"><h2 id="缺点" tabindex="-1"><a class="header-anchor" href="#缺点" aria-hidden="true">#</a> 缺点</h2><p>词向量表训练好了之后就不再变化了，比如训练语料中，只有关于 apple 是我们所吃的苹果这种意思时，当遇到苹果手机中的苹果含义时，就无法表示其真正的表征。</p><h2 id="item2vec" tabindex="-1"><a class="header-anchor" href="#item2vec" aria-hidden="true">#</a> Item2vec</h2><p>Item2vec其实是将Word2vec算法用在了推荐系统上，与word2vec的区别在于，Item2vec没有时间窗口的概念，认为序列中任意两个物品都可以相关，所以其目标函数为：</p>',11),B=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mfrac",null,[s("mn",null,"1"),s("mi",null,"K")]),s("munderover",null,[s("mo",null,"∑"),s("mrow",null,[s("mi",null,"i"),s("mo",null,"="),s("mn",null,"1")]),s("mi",null,"K")]),s("munderover",null,[s("mo",null,"∑"),s("mrow",null,[s("mi",null,"j"),s("mo",{mathvariant:"normal"},"≠"),s("mi",null,"i")]),s("mi",null,"K")]),s("mi",null,"log"),s("mo",null,"⁡"),s("mrow",null,[s("mi",null,"p"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"w"),s("mi",null,"j")]),s("mi",{mathvariant:"normal"},"∣"),s("msub",null,[s("mi",null,"w"),s("mi",null,"i")]),s("mo",{stretchy:"false"},")")])]),s("annotation",{encoding:"application/x-tex"}," \\frac{1}{K}\\sum_{i=1}^K\\sum_{j\\neq i}^K\\log{p(w_j|w_i)} ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"3.2666em","vertical-align":"-1.4382em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.3214em"}},[s("span",{style:{top:"-2.314em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"K")])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.677em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.686em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mop op-limits"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.8283em"}},[s("span",{style:{top:"-1.8723em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mrel mtight"},"="),s("span",{class:"mord mtight"},"1")])])]),s("span",{style:{top:"-3.05em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",null,[s("span",{class:"mop op-symbol large-op"},"∑")])]),s("span",{style:{top:"-4.3em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.07153em"}},"K")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.2777em"}},[s("span")])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mop op-limits"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.8283em"}},[s("span",{style:{top:"-1.8479em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"j"),s("span",{class:"mrel mtight"},[s("span",{class:"mrel mtight"},[s("span",{class:"mord vbox mtight"},[s("span",{class:"thinbox mtight"},[s("span",{class:"rlap mtight"},[s("span",{class:"strut",style:{height:"0.8889em","vertical-align":"-0.1944em"}}),s("span",{class:"inner"},[s("span",{class:"mord mtight"},[s("span",{class:"mrel mtight"},"")])]),s("span",{class:"fix"})])])])]),s("span",{class:"mrel mtight"},"=")]),s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{style:{top:"-3.05em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",null,[s("span",{class:"mop op-symbol large-op"},"∑")])]),s("span",{style:{top:"-4.3em","margin-left":"0em"}},[s("span",{class:"pstrut",style:{height:"3.05em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.07153em"}},"K")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.4382em"}},[s("span")])])])]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mop"},[a("lo"),s("span",{style:{"margin-right":"0.01389em"}},"g")]),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.02691em"}},"w"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0269em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05724em"}},"j")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])]),s("span",{class:"mord"},"∣"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.02691em"}},"w"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0269em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")")])])])])])],-1),I=e('<p>但是Item2vec算法在处理互联网场景下<strong>大量的网络化数据</strong>时往往显得捉襟见肘，因为：</p><p><strong>数据稀疏性</strong>：</p><ul><li>在互联网场景中，项目之间的关联网络通常是稀疏的，即很多项目之间没有直接的连接或交互。这会导致学习到的嵌入表示可能无法很好地捕捉项目之间的关联和相似性。</li></ul><p><strong>网络结构复杂性</strong>：</p><ul><li>互联网场景中的网络结构通常非常复杂，包含大量的项目和用户之间的交互关系。Item2Vec 算法在处理复杂网络结构时可能无法充分挖掘项目之间的潜在关系，导致学习到的嵌入表示可能不够准确。</li></ul><p><strong>数据质量和噪声</strong>：</p><ul><li>互联网场景中的数据通常存在质量问题和噪声，如缺失值、异常值、重复数据等。这些问题会影响 Item2Vec 算法的性能，使得学习到的嵌入表示可能受到干扰和误导。</li></ul><p>其实还可以这样总结：Item2vec和Word2vec都是建立在序列样本的基础上的，在互联网场景下，数据对象之间更多呈现的是图结构，直接迁移过来使用不一定很合适。</p><h2 id="相关面试题" tabindex="-1"><a class="header-anchor" href="#相关面试题" aria-hidden="true">#</a> 相关面试题</h2><p><strong>Wordvec中CBOW与Skip-Gram是什么？</strong></p><ul><li><p>CBOW</p><p>思想：用周围词预测中心词</p><p>输入输出介绍：输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。</p></li><li><p>Skip-gram</p><p>思想：用中心词预测周围词</p><p>输入输出介绍：输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量</p></li></ul><p><strong>Word2vec中霍夫曼树是什么？</strong></p><ul><li><p>霍夫曼树根据单词出现的频率构建，频率较高的单词在霍夫曼树中被赋予较短的编码，频率较低的单词被赋予较长的编码，从而实现了对词向量的高效表示和存储。</p></li><li><p>选择两个频率最低的节点，将它们合并为一个新的节点，并将新节点的频率设置为原节点频率的和。</p></li><li><p>霍夫曼树的特点是频率较高的单词被分配到较短的编码，频率较低的单词被分配到较长的编码，以实现对词向量的高效编码和存储。在 Word2Vec 中，霍夫曼树的构建过程用于生成每个单词的 Huffman 编码，从而得到单词的紧凑表示，用于训练词向量模型。</p></li></ul><p><strong>word2vec和NNLM（神经网络语言模型）的区别？</strong></p><ul><li><p>word2vec为了得到词向量，有大量优化：对输入的词向量直接按列求和，再按列求平均，这样输入的多个词向量就变成了一个词向量；采用分层的 softmax(hierarchical softmax)，实质上是一棵哈夫曼树；采用负采样，从所有的单词中采样出指定数量的单词，而不需要使用全部的单词</p></li><li><p>NNLM为了预测下一个词 (使用前 n - 1 个单词预测第 n 个单词)</p></li></ul>',15);function j(K,N){const t=m("ExternalLinkIcon");return p(),r("div",null,[s("div",b,[f,w,s("p",null,[s("a",x,[a("5-负采样方案_哔哩哔哩_bilibili"),l(t)])]),s("p",null,[s("a",k,[a("06 Word2Vec模型（第一个专门做词向量的模型，CBOW和Skip-gram）_哔哩哔哩_bilibili"),l(t)])]),s("p",null,[s("a",z,[a("理解 Word2Vec 之 Skip-Gram 模型 - 知乎 (zhihu.com)"),l(t)])])]),M,L,V,W,B,I])}const q=i(_,[["render",j],["__file","word2vec.html.vue"]]);export{q as default};
