import{_ as h,r as i,o,c as s,a as e,b as a,d as n,e as r}from"./app-aa9cafec.js";const l="/MLblogs/assets/2024-03-22-17-40-55-image-a22e2e90.png",c="/MLblogs/assets/2024-03-22-16-57-51-image-850e43f8.png",d={},p=e("p",null,"ChatGLM的好处：支持国产GPU、可以小型化部署、效果不错、生态完整。",-1),_=e("p",null,"ChatGLM-6B: 62亿参数",-1),g=e("p",null,"经过大约1T标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持。",-1),u={href:"https://www.heywhale.com/mw/project/6436d82948f7da1fee2be59e",target:"_blank",rel:"noopener noreferrer"},m=r('<h2 id="chatglm的训练" tabindex="-1"><a class="header-anchor" href="#chatglm的训练" aria-hidden="true">#</a> ChatGLM的训练</h2><img src="'+l+'" title="" alt="" data-align="center"><p>和Bert的共同点：都是预测被mask掉的单词</p><p>不同点：一段话中，Bert只mask一处，而ChatGLM可以连续mask多个单词，预测被Mask的片段的开始和结束位置。</p><p>Bert不能做生成任务。</p><h2 id="webglm" tabindex="-1"><a class="header-anchor" href="#webglm" aria-hidden="true">#</a> WebGLM</h2><img src="'+c+'" title="" alt="" data-align="center"><h2 id="微调实践" tabindex="-1"><a class="header-anchor" href="#微调实践" aria-hidden="true">#</a> 微调实践</h2>',8),b={href:"https://github.com/THUDM/ChatGLM-6B/tree/main",target:"_blank",rel:"noopener noreferrer"},f={href:"https://github.com/liucongg/ChatGLM-Finetuning/tree/master",target:"_blank",rel:"noopener noreferrer"},L=e("h3",{id:"环境配置",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#环境配置","aria-hidden":"true"},"#"),a(" 环境配置")],-1),M={href:"https://huggingface.co/THUDM/chatglm-6b/tree/main",target:"_blank",rel:"noopener noreferrer"},G=e("h3",{id:"数据集",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#数据集","aria-hidden":"true"},"#"),a(" 数据集")],-1),C={href:"https://tianchi.aliyun.com/dataset/86895",target:"_blank",rel:"noopener noreferrer"},k=r('<h3 id="使用chatglm-6b官方仓库作为base微调" tabindex="-1"><a class="header-anchor" href="#使用chatglm-6b官方仓库作为base微调" aria-hidden="true">#</a> 使用ChatGLM-6B官方仓库作为base微调</h3><h3 id="使用chatglm-finetuning仓库作为base微调" tabindex="-1"><a class="header-anchor" href="#使用chatglm-finetuning仓库作为base微调" aria-hidden="true">#</a> 使用ChatGLM-Finetuning仓库作为base微调</h3><h3 id="评估方案" tabindex="-1"><a class="header-anchor" href="#评估方案" aria-hidden="true">#</a> 评估方案</h3>',3),x={href:"https://zhuanlan.zhihu.com/p/620885226",target:"_blank",rel:"noopener noreferrer"},B=r("<p>由于生成模型的内容不能想信息抽取任务一样评价，用现有的BLUE或者Rouge来评价也是不合适，因此制定了评分规则。 通过多样性和准确性两个角度判断D2Q模型好坏，每个样本总计5分，共20个样本。</p><p>多样性：</p><ul><li><p>问题是否高度相似，每重复一个问题扣0.25分；</p></li><li><p>问题对应答案是否相同，每有一个重复答案或找不到答案，扣0.25分；</p></li></ul><p>准确性：</p><ul><li><p>问题能否从文档中找到答案，每有一个找不到答案，扣0.25分；</p></li><li><p>问题内容是否流畅，每有一个问题不流畅，扣0.25分；</p></li><li><p>问题内容是否有害，每有一个有害，扣0.25分；</p></li></ul>",5);function w(T,v){const t=i("ExternalLinkIcon");return o(),s("div",null,[p,_,g,e("p",null,[e("a",u,[a("ChatGLM-6B 在 ModelWhale 平台的部署与微调教程 - Heywhale.com"),n(t)])]),m,e("p",null,[a("微调的时候，可以使用"),e("a",b,[a("ChatGLM-6B的官方仓库"),n(t)]),a("为base，也可以使用"),e("a",f,[a("ChatGLM-Finetuning"),n(t)]),a("作为base来微调，官方仓库目前的版本（2024/3/24）只有基于P-Tuning v2方法的微调；而后者实现了Freeze方法、Lora方法、P-Tuning方法、全量参数等的微调。")]),L,e("p",null,[a("先将Huggingface中，ChatGLM的权重下载下来，这里下载"),e("a",M,[a("ChatGLM-6B"),n(t)]),a("，将所有文件都下载下来，放在chatglm-6b的文件夹内。")]),G,e("p",null,[e("a",C,[a("中医文献问题生成数据集_数据集-阿里云天池 (aliyun.com)"),n(t)])]),k,e("p",null,[a("参考："),e("a",x,[a("大模型LLM-微调经验分享&总结 - 知乎 (zhihu.com)"),n(t)])]),B])}const z=h(d,[["render",w],["__file","ChatGLM.html.vue"]]);export{z as default};
