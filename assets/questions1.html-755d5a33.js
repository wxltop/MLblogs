const e=JSON.parse('{"key":"v-7f27b704","path":"/docs/transformer/questions1.html","title":"transformer面试问题","lang":"en-US","frontmatter":{"title":"transformer面试问题","date":"2024/3/17","categories":["transformer"],"tags":["transformer"]},"headers":[{"level":2,"title":"自注意机制计算为何要除以?","slug":"自注意机制计算为何要除以","link":"#自注意机制计算为何要除以","children":[]},{"level":2,"title":"为何使用多头注意力机制？","slug":"为何使用多头注意力机制","link":"#为何使用多头注意力机制","children":[]},{"level":2,"title":"为什么Q和K使用不同的权重矩阵生成？","slug":"为什么q和k使用不同的权重矩阵生成","link":"#为什么q和k使用不同的权重矩阵生成","children":[]},{"level":2,"title":"计算attention的时候为何选择点乘？","slug":"计算attention的时候为何选择点乘","link":"#计算attention的时候为何选择点乘","children":[]},{"level":2,"title":"为什么softmax之前需要对attention进行scaled？","slug":"为什么softmax之前需要对attention进行scaled","link":"#为什么softmax之前需要对attention进行scaled","children":[]},{"level":2,"title":"why多头注意力的时候需要对每个head降维？","slug":"why多头注意力的时候需要对每个head降维","link":"#why多头注意力的时候需要对每个head降维","children":[]},{"level":2,"title":"位置编码的作用？","slug":"位置编码的作用","link":"#位置编码的作用","children":[]},{"level":2,"title":"其他位置编码的技术，各自的优缺点是什么？","slug":"其他位置编码的技术-各自的优缺点是什么","link":"#其他位置编码的技术-各自的优缺点是什么","children":[]},{"level":2,"title":"Transformer中的残差结构以及意义？","slug":"transformer中的残差结构以及意义","link":"#transformer中的残差结构以及意义","children":[]},{"level":2,"title":"why transformer中使用LN而不是BN？","slug":"why-transformer中使用ln而不是bn","link":"#why-transformer中使用ln而不是bn","children":[]},{"level":2,"title":"简答讲一下BatchNorm技术，以及它的优缺点","slug":"简答讲一下batchnorm技术-以及它的优缺点","link":"#简答讲一下batchnorm技术-以及它的优缺点","children":[]},{"level":2,"title":"FFN的作用？激活函数？","slug":"ffn的作用-激活函数","link":"#ffn的作用-激活函数","children":[]},{"level":2,"title":"ReLU为何作为激活函数？","slug":"relu为何作为激活函数","link":"#relu为何作为激活函数","children":[]},{"level":2,"title":"Transformer和RNN、LSTM的区别？","slug":"transformer和rnn、lstm的区别","link":"#transformer和rnn、lstm的区别","children":[]},{"level":2,"title":"Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？","slug":"transformer的并行化提现在哪个地方-decoder端可以做并行化吗","link":"#transformer的并行化提现在哪个地方-decoder端可以做并行化吗","children":[]},{"level":2,"title":"LN的作用？","slug":"ln的作用","link":"#ln的作用","children":[]},{"level":2,"title":"RNN 和 LSTM 在处理长序列时可能会出现梯度消失或梯度爆炸等问题？","slug":"rnn-和-lstm-在处理长序列时可能会出现梯度消失或梯度爆炸等问题","link":"#rnn-和-lstm-在处理长序列时可能会出现梯度消失或梯度爆炸等问题","children":[]},{"level":2,"title":"如何防止 Transformer 模型过拟合?","slug":"如何防止-transformer-模型过拟合","link":"#如何防止-transformer-模型过拟合","children":[]},{"level":2,"title":"transformer的dropout层通常用在什么地方?","slug":"transformer的dropout层通常用在什么地方","link":"#transformer的dropout层通常用在什么地方","children":[]},{"level":2,"title":"Transformer 模型对于输入序列的长度有何限制？如果要处理很长的序列，采取什么措施？","slug":"transformer-模型对于输入序列的长度有何限制-如果要处理很长的序列-采取什么措施","link":"#transformer-模型对于输入序列的长度有何限制-如果要处理很长的序列-采取什么措施","children":[]},{"level":2,"title":"什么类型的任务中可能会偏好其他模型而不是 Transformer？","slug":"什么类型的任务中可能会偏好其他模型而不是-transformer","link":"#什么类型的任务中可能会偏好其他模型而不是-transformer","children":[]},{"level":2,"title":"如何理解Transformer的决策过程？","slug":"如何理解transformer的决策过程","link":"#如何理解transformer的决策过程","children":[]}],"git":{"createdTime":null,"updatedTime":null,"contributors":[]},"filePathRelative":"docs/transformer/questions1.md"}');export{e as data};
