import{_ as a,r as i,o as p,c as s,a as n,b as e,d as r,e as o}from"./app-aa9cafec.js";const h="/MLblogs/assets/2024-03-28-11-49-32-image-6212c704.png",l="/MLblogs/assets/2024-03-28-14-03-02-image-d9536124.png",c="/MLblogs/assets/2024-03-28-14-04-35-image-9b0ec16a.png",d="/MLblogs/assets/2024-03-28-14-01-14-image-57919b39.png",g="/MLblogs/assets/2024-03-28-14-21-05-image-7f2b3edc.png",u="/MLblogs/assets/2024-03-28-14-33-54-image-d8630aa0.png",m={},_={class:"custom-container info"},f=n("svg",{xmlns:"http://www.w3.org/2000/svg","xmlns:xlink":"http://www.w3.org/1999/xlink",viewBox:"0 0 24 24"},[n("g",{fill:"none",stroke:"currentColor","stroke-width":"2","stroke-linecap":"round","stroke-linejoin":"round"},[n("circle",{cx:"12",cy:"12",r:"9"}),n("path",{d:"M12 8h.01"}),n("path",{d:"M11 12h1v4h1"})])],-1),x=n("p",{class:"custom-container-title"},"INFO",-1),P={href:"https://zhuanlan.zhihu.com/p/636481171",target:"_blank",rel:"noopener noreferrer"},k=o('<h2 id="lora" tabindex="-1"><a class="header-anchor" href="#lora" aria-hidden="true">#</a> LoRA</h2><p>模型是过参数化的，它们有更小的内在维度，模型主要依赖于这个低的内在维度（low intrinsic dimension）去做任务适配。</p><p>首先确定要微调的权重矩阵。通常，这些矩阵位于模型的多头自注意力（Multi-head Self-Attention）和前馈神经网络（Feed-Forward Neural Network）部分。</p><p>引入两个低秩矩阵，记为A和B，这两个矩阵的维度比原始权重矩阵小得多。假设原始矩阵维度为dxd，A和B的维度分别为dxr和rxd，其中r&lt;&lt;d。其中A利用随机高斯分布初始化，B初始化为全0。</p><h2 id="qlora" tabindex="-1"><a class="header-anchor" href="#qlora" aria-hidden="true">#</a> QLoRA</h2><p>在LoRA（Low-Rank Adaptation）的基础上引入了深度量化过程。QLoRA使用一种新颖的高精度技术将预训练模型量化为4-bit。这种技术包括一种低精度存储数据类型（4-bit NormalFloat，简写为NF4）和一种计算数据类型（16-bit BrainFloat）。</p><h2 id="adapter" tabindex="-1"><a class="header-anchor" href="#adapter" aria-hidden="true">#</a> Adapter</h2><p>在这个预训练模型的每一层或选定的层中，我们插入适配器。这些适配器是小型的神经网络，通常只包含几层，并且参数相对较少。</p><p>通过添加Adapter模块来避免全模型微调与灾难性遗忘的问题。</p><p><strong>LoRA和Adapter Tuning的区别：</strong></p><p><strong>LoRA</strong>：通过在模型的权重矩阵中引入低秩矩阵（通常是两个小的矩阵的乘积）来实现对模型的微调。这些低秩矩阵作为原有权重矩阵的修改项，使得原有的权重矩阵在实际计算时得到调整。</p><p><strong>Adapter Tuning</strong>：通过在模型的各个层中添加小型神经网络模块，即“适配器”，来实现微调。这些适配器独立于模型的主体结构，只有它们的参数在微调过程中被更新，而模型的其他预训练参数保持不变。</p><h2 id="prefix-tuning" tabindex="-1"><a class="header-anchor" href="#prefix-tuning" aria-hidden="true">#</a> Prefix-tuning</h2><p>对于每个任务，都有一个特定的前缀被添加到输入序列的开始部分。这些前缀相当于任务特定的提示，可以是一组固定的词或是可训练的嵌入向量。</p>',14),b={href:"https://zhuanlan.zhihu.com/p/639685912",target:"_blank",rel:"noopener noreferrer"},L=o('<p>prefix tuning将prefix参数（可训练的张量）添加到所有的transformer层</p><p>机制：将多个prompt vectors 放在每个multi-head attention的key矩阵和value矩阵之前</p><p>计算方式：相当于原始的token要多和这些soft prompt token计算相似度，然后聚合。</p><img src="'+h+'" title="" alt="" data-align="center"><p>通过上面等式的变换，等式的前部分是不加入prefix向量的初始attention计算的公式，后半部分则是上下文向量无关的部分。通过一个类似门的机制来计算前后两部分的比重</p><p>如果用h表示原本的attention模块输出，加入prefix的attention模块输出等于原本attention模型输出和一个与上下文无关的增量之间的加权平均。</p><img src="'+l+'" title="" alt="" data-align="center"><img src="'+c+'" title="" alt="" data-align="center"><p><strong>prefix部分到底使用多少个虚拟token</strong>，直接影响模型微调的参数量级，以及处理长文本的能力。<strong>默认的prefix长度为10</strong>，作者在不同任务上进行了微调，<strong>整体上prompt部分的参数量都在原模型的~0.1%</strong>。</p><h2 id="prompt-tuning" tabindex="-1"><a class="header-anchor" href="#prompt-tuning" aria-hidden="true">#</a> Prompt Tuning</h2>',10),T={href:"https://zhuanlan.zhihu.com/p/632009060",target:"_blank",rel:"noopener noreferrer"},A={href:"https://magazine.sebastianraschka.com/p/understanding-parameter-efficient",target:"_blank",rel:"noopener noreferrer"},v=o('<p>可以看作是 Prefix Tuning 的简化版本，只在输入层加入 prompt tokens。随着预训练模型参数量的增加，Prompt Tuning的方法会逼近 Fine-tune 的结果。</p><p><em>硬</em>提示调整（<em>hard</em> prompt tuning），因为我们直接更改不可微分的离散输入标记。与<em>硬</em>提示调整相反，<em>软</em>提示调整（<em>soft</em> prompt tuning）将输入标记的嵌入与可训练张量连接起来，该张量可以通过反向传播进行优化，以提高目标任务的建模性能。</p><p>Prompt 长度影响：模型参数达到一定量级时，Prompt 长度为1也能达到不错的效果，Prompt 长度为20就能达到极好效果。</p><img src="'+d+'" title="" alt="" data-align="center"><h2 id="p-tuning" tabindex="-1"><a class="header-anchor" href="#p-tuning" aria-hidden="true">#</a> P-Tuning</h2>',5),M={href:"https://zhuanlan.zhihu.com/p/635848732",target:"_blank",rel:"noopener noreferrer"},w=o('<p>相比Prefix Tuning，P-Tuning加入的可微的virtual token，但仅限于输入层，没有在每一层都加；另外，virtual token的位置也不一定是前缀，插入的位置是可选的。这里的出发点实际是把传统人工设计模版中的真实token替换成可微的virtual token。</p><img src="'+g+'" title="" alt="" data-align="center"><p>作者通过实验发现用一个prompt encoder来编码会收敛更快，效果更好。即用一个LSTM+MLP去编码这些virtual token以后，再输入到模型。</p><p><strong>和prompt tuning的区别？</strong></p><p>p-tuning将virsual token经过编码器（LSTM+MLP），然后插入到输入文本token序列中，如何插入？不同的模型有不同的模板：对于BERT类双向语言模型采用模版<code>(P1, x, P2, [MASK], P3)</code>，对于单向语言模型采用<code>(P1, x, P2, [MASK])</code>。</p><p>prompt tuning直接使用多个可学习的prompt tokens（可学习）拼接在输入序列的tokens上。长度和单词token长度相同。</p><h2 id="p-tuning-v2" tabindex="-1"><a class="header-anchor" href="#p-tuning-v2" aria-hidden="true">#</a> P-Tuning v2</h2><p>该方法在每一层都加入了Prompts tokens作为输入，而不是仅仅加在输入层，这带来两个方面的好处：</p><ul><li>更多可学习的参数（从P-tuning和Prompt Tuning的0.01%增加到0.1%-3%），同时也足够参数高效。</li><li>加入到更深层结构中的Prompt能给模型预测带来更直接的影响。</li></ul><img src="'+u+'" title="" alt="" data-align="center"><p>具体做法基本同Prefix Tuning，可以看作是将文本生成的Prefix Tuning技术适配到NLU任务中。然后做了一些改进：</p><ul><li><p><strong>移除重参数化的编码器</strong>。以前的方法利用重参数化功能来提高训练速度和鲁棒性（如：Prefix Tuning中的MLP、P-Tuning中的LSTM））。在 P-tuning v2 中，作者发现重参数化的改进很小，尤其是对于较小的模型，同时还会影响模型的表现。</p></li><li><p><strong>针对不同任务采用不同的提示长度</strong>。提示长度在提示优化方法的超参数搜索中起着核心作用。在实验中，我们发现不同的理解任务通常用不同的提示长度来实现其最佳性能</p></li></ul><h2 id="torchtune库" tabindex="-1"><a class="header-anchor" href="#torchtune库" aria-hidden="true">#</a> torchtune库</h2>',13),z={href:"https://zhuanlan.zhihu.com/p/688671130",target:"_blank",rel:"noopener noreferrer"};function N(B,R){const t=i("ExternalLinkIcon");return p(),s("div",null,[n("div",_,[f,x,n("p",null,[n("a",P,[e("大模型微调（finetune）方法总结-LoRA,Adapter,Prefix-tuning，P-tuning，Prompt-tuning - 知乎 (zhihu.com)"),r(t)])])]),k,n("p",null,[n("a",b,[e("深入理解Prefix Tuning - 知乎 (zhihu.com)"),r(t)])]),L,n("p",null,[n("a",T,[e("LLM微调方法：Prompt Tuning And Prefix Tuning - 知乎 (zhihu.com)"),r(t)])]),n("p",null,[n("a",A,[e("Understanding Parameter-Efficient LLM Finetuning: Prompt Tuning And Prefix Tuning (sebastianraschka.com)"),r(t)])]),v,n("p",null,[n("a",M,[e("大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2 - 知乎 (zhihu.com)"),r(t)])]),w,n("p",null,[n("a",z,[e("PyTorch官方发布LLM微调工具TorchTune - 知乎 (zhihu.com)"),r(t)])])])}const S=a(m,[["render",N],["__file","weidiao.html.vue"]]);export{S as default};
