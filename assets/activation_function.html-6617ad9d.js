import{_ as t,o as n,c as e,e as a,a as s,b as l}from"./app-aa9cafec.js";const i="/MLblogs/imgs/2023-09-26-13-09-30-image.png",m="/MLblogs/assets/2023-09-26-13-11-09-image-da7dc133.png",r="/MLblogs/assets/2023-09-26-13-13-02-image-65b27f37.png",p="/MLblogs/assets/2023-09-26-13-14-17-image-17ad6d89.png",c="/MLblogs/assets/2023-09-26-13-15-26-image-4055cf82.png",o="/MLblogs/assets/2023-09-26-13-16-33-image-b1b5f176.png",h="/MLblogs/assets/2023-09-26-13-17-42-image-f0f4aee0.png",g="/MLblogs/assets/2023-09-26-13-22-51-image-f294a9fd.png",d="/MLblogs/assets/2023-09-26-13-23-54-image-9a41a2c0.png",u="/MLblogs/assets/2023-09-26-13-30-40-image-706e8d03.png",x="/MLblogs/assets/2023-09-26-13-31-48-image-d728fe6a.png",y="/MLblogs/assets/2023-09-26-13-33-06-image-c4519892.png",f={},L=a('<h2 id="激活函数的特性" tabindex="-1"><a class="header-anchor" href="#激活函数的特性" aria-hidden="true">#</a> 激活函数的特性？</h2><ol><li><p><strong>非线性性质：</strong> 激活函数必须是非线性的，否则多个线性层的组合仍然会等效于一个线性层。非线性激活函数使神经网络能够学习和表示更复杂的函数。</p></li><li><p><strong>可微性（几乎处处可微）：</strong> 大多数神经网络训练算法依赖于梯度下降等优化方法，因此激活函数最好是可微的。尽管ReLU在x=0处不可导，但在实践中可以使用次梯度。</p></li><li><p><strong>单调性：</strong> 单调激活函数有助于减少训练复杂性，并且更容易优化。不过，非单调激活函数如Swish和Hardtanh也在实际中使用。</p></li><li><p><strong>输出范围：</strong> 激活函数的输出范围通常应该在某个有界区间内，以限制神经元输出的幅度。这可以有助于稳定训练。</p></li></ol><p><strong>常用激活函数：</strong></p><ol><li><p><strong>Sigmoid函数：</strong> 将输入映射到0和1之间，常用于二分类问题或在某些循环神经网络中。</p></li><li><p><strong>Tanh函数：</strong> 将输入映射到-1和1之间，具有零中心化特性，有助于减少梯度消失问题。</p></li><li><p><strong>ReLU函数（Rectified Linear Unit）：</strong> f(x) = max(0, x)。ReLU是最常用的激活函数之一，因为它简单且有效。它引入了非线性性质，但在x=0处不可导，通常使用次梯度来处理。</p></li><li><p><strong>Leaky ReLU函数：</strong> 修正了ReLU的不足，当x&lt;0时引入了小的负斜率，以解决死亡神经元问题。</p></li><li><p><strong>PReLU函数（Parametric ReLU）：</strong> 类似于Leaky ReLU，但斜率是可学习的。</p></li><li><p><strong>SiLU函数（Sigmoid Linear Unit）：</strong> f(x) = x * sigmoid(x)。SiLU是近年来提出的激活函数，具有更平滑的梯度，有时被称为Swish函数的变种。</p></li><li><p><strong>ELU函数（Exponential Linear Unit）：</strong> 引入了指数项，允许负数输入时仍然产生有限的激活值，有助于减少梯度消失问题。</p></li><li><p><strong>SELU函数（Scaled Exponential Linear Unit）：</strong> 是ELU的变体，具有自归一化的性质，有助于稳定训练。</p></li><li><p><strong>Swish函数：</strong> 与SiLU类似，但包含一个可调参数，可以控制函数的形状。</p></li><li><p><strong>Softplus函数：</strong> 是Sigmoid函数的平滑版本，通常用于回归任务和一些概率模型。</p></li><li><p><strong>Hardtanh函数：</strong> 带有硬界限的激活函数，类似于Tanh，但在一定范围内输出线性函数。</p></li></ol><h2 id="sigmoid" tabindex="-1"><a class="header-anchor" href="#sigmoid" aria-hidden="true">#</a> Sigmoid</h2>',5),b=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",null,"σ"),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mfrac",null,[s("mn",null,"1"),s("mrow",null,[s("mn",null,"1"),s("mo",null,"+"),s("msup",null,[s("mi",null,"e"),s("mrow",null,[s("mo",null,"−"),s("mi",null,"x")])])])])]),s("annotation",{encoding:"application/x-tex"}," \\sigma(x)=\\frac{1}{1+e^{-x}} ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"σ"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"2.0908em","vertical-align":"-0.7693em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.3214em"}},[s("span",{style:{top:"-2.314em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"1"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"e"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6973em"}},[s("span",{style:{top:"-2.989em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"−"),s("span",{class:"mord mathnormal mtight"},"x")])])])])])])])])])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.677em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7693em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})])])])])])],-1),_=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",null,"σ"),s("mo",{mathvariant:"normal"},"′"),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mi",null,"σ"),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",{stretchy:"false"},")"),s("mo",{stretchy:"false"},"("),s("mn",null,"1"),s("mo",null,"−"),s("mi",null,"σ"),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",{stretchy:"false"},")"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"}," \\sigma\\prime(x)=\\sigma(x)(1-\\sigma(x)) ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"σ"),s("span",{class:"mord"},"′"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"σ"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mclose"},")"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},"1"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"−"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"σ"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mclose"},"))")])])])])],-1),U=a('<img src="'+i+'" title="" alt="" data-align="center"><ul><li><p>为何sigmoid可以作为激活函数，但是并不是在网络中间层经常使用？</p><p>sigmoid首先满足作为激活函数的那些特点，其次<strong>它的导数可以用它自己来表示</strong> ，使得当使用反向传播算法时，很容易推导出学习神经网络中的权重的更新方程。但是由于以下的一些问题，使得sigmoid在网络中间层并不被常用：</p><ol><li><p><strong>饱和性质：</strong> Sigmoid函数在它的输入极值处（接近正无穷或负无穷）饱和，意味着它的导数趋近于零。这导致了梯度消失问题，尤其是在深度网络中。当梯度接近零时，神经网络的权重更新会非常缓慢，导致训练变得困难。</p></li><li><p><strong>输出范围：</strong> Sigmoid函数的输出范围在0到1之间，因此输出值不是零中心化的。这可能导致梯度的不稳定性，因为在反向传播时，梯度可能都是正的或负的，这可能会导致权重更新的震荡。</p></li><li><p><strong>计算成本：</strong> 相对于一些其他激活函数，如ReLU，Sigmoid函数的计算成本较高，因为它涉及指数运算。</p></li></ol><p>尽管Sigmoid函数在中间层存在一些问题，但它在二分类问题的输出层中仍然很有用，因为它将输出映射到0到1的概率值，适用于二分类问题的概率估计。此外，它在一些特定的神经网络体系结构，如循环神经网络（RNN）中也有一定用途。</p></li></ul><h2 id="softmax" tabindex="-1"><a class="header-anchor" href="#softmax" aria-hidden="true">#</a> Softmax</h2><img src="'+m+'" title="" alt="" data-align="center"><ul><li><p>为什么softmax需要先对每一个数执行指数操作，然后再求比值？</p><ol><li><p><strong>非线性变换：</strong> Softmax函数对输入进行指数操作，将每个输入值的幂指数化。这个指数操作是一个非线性变换，它增加了输入值之间的差异，突出了最大的输入值，并抑制了较小的输入值。这使得模型更容易区分具有不同置信度的类别。</p></li><li><p><strong>概率分布：</strong> Softmax函数的输出值总和为1，因此将原始输入值映射到一个概率分布上。每个输出值表示对应类别的概率。这使得Softmax函数在多类别分类任务中非常有用，因为您可以根据概率来决定最可能的类别。</p></li><li><p><strong>梯度性质：</strong> Softmax函数具有很好的梯度性质，使得它适用于梯度下降等优化算法。在反向传播时，可以方便地计算每个类别的梯度，以更新模型的权重。</p></li><li><p><strong>数值稳定性：</strong> Softmax函数的指数操作可以提高数值稳定性。在指数操作之前，输入可能具有各种值范围，指数操作可以将这些值压缩到一个合理的范围内，避免数值上溢或下溢问题。</p></li></ol></li></ul><h2 id="relu" tabindex="-1"><a class="header-anchor" href="#relu" aria-hidden="true">#</a> ReLU</h2>',6),v=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",null,"R"),s("mi",null,"e"),s("mi",null,"L"),s("mi",null,"U"),s("mo",null,"="),s("mi",null,"m"),s("mi",null,"a"),s("mi",null,"x"),s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",{separator:"true"},","),s("mn",null,"0"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"}," ReLU=max(x, 0) ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.00773em"}},"R"),s("span",{class:"mord mathnormal"},"e"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.10903em"}},"LU"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"ma"),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},"0"),s("span",{class:"mclose"},")")])])])])],-1),S=s("img",{src:r,title:"",alt:"","data-align":"center"},null,-1),k=s("ul",null,[s("li",null,[s("p",null,"ReLU在x=0处是否可微？"),s("p",null,[l("不是，"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"f"),s("mo",{mathvariant:"normal"},"′"),s("mo",{stretchy:"false"},"("),s("msup",null,[s("mi",null,"x"),s("mo",null,"−")]),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mn",null,"0"),s("mo",{mathvariant:"normal"},"≠"),s("mi",null,"f"),s("mo",{mathvariant:"normal"},"′"),s("mo",{stretchy:"false"},"("),s("msup",null,[s("mi",null,"x"),s("mo",null,"+")]),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mn",null,"1")]),s("annotation",{encoding:"application/x-tex"},"f\\prime(x^-)=0\\neq f\\prime(x^+)=1")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0213em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f"),s("span",{class:"mord"},"′"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"x"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7713em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mbin mtight"},"−")])])])])])])]),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8889em","vertical-align":"-0.1944em"}}),s("span",{class:"mord"},"0"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},[s("span",{class:"mrel"},[s("span",{class:"mord vbox"},[s("span",{class:"thinbox"},[s("span",{class:"rlap"},[s("span",{class:"strut",style:{height:"0.8889em","vertical-align":"-0.1944em"}}),s("span",{class:"inner"},[s("span",{class:"mord"},[s("span",{class:"mrel"},"")])]),s("span",{class:"fix"})])])])]),s("span",{class:"mrel"},"=")]),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0213em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f"),s("span",{class:"mord"},"′"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"x"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7713em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mbin mtight"},"+")])])])])])])]),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"1")])])]),l("，所以不可微")])]),s("li",null,[s("p",null,"既然不是处处可微，ReLU为何可以作为激活函数？实际如何处理的？"),s("p",null,"当在反向传播中遇到x=0时，可以将ReLU函数的导数视为介于0和1之间的任何值，通常选择0.5作为近似值。这意味着梯度在x=0处会被缩小一半，但不会消失。这种近似足够使得神经网络能够进行有效的训练。"),s("p",null,"如果对ReLU的不可微性担忧，可以考虑使用Leaky ReLU、Parametric ReLU（PReLU）或其他激活函数的变体，这些函数在x=0处引入了小的负斜率，从而解决了不可微性问题，同时保留了ReLU的非线性特性。")])],-1),w=a('<h2 id="silu" tabindex="-1"><a class="header-anchor" href="#silu" aria-hidden="true">#</a> SiLU</h2><img src="'+p+'" title="" alt="" data-align="center"><h2 id="prelu" tabindex="-1"><a class="header-anchor" href="#prelu" aria-hidden="true">#</a> PReLU</h2><img src="'+c+'" title="" alt="" data-align="center"><h2 id="leaky-relu" tabindex="-1"><a class="header-anchor" href="#leaky-relu" aria-hidden="true">#</a> Leaky ReLU</h2><img src="'+o+'" title="" alt="" data-align="center"><h2 id="tanh" tabindex="-1"><a class="header-anchor" href="#tanh" aria-hidden="true">#</a> Tanh</h2><img src="'+h+'" title="" alt="" data-align="center"><h2 id="elu" tabindex="-1"><a class="header-anchor" href="#elu" aria-hidden="true">#</a> ELU</h2><p>ELU（Exponential Linear Unit）函数是一种平滑的非线性函数，可以克服ReLU函数的一些问题。它包含一个指数项，允许负数输入时仍然产生有限的激活值，并且具有负数部分的导数不为零，有助于减少梯度消失问题。</p><img src="'+g+'" title="" alt="" data-align="center"><h2 id="selu" tabindex="-1"><a class="header-anchor" href="#selu" aria-hidden="true">#</a> SELU</h2><p>SELU（Scaled Exponential Linear Unit）SELU函数是ELU的变体，具有自归一化的性质，可以用于深度神经网络，有助于稳定训练。</p><img src="'+d+'" title="" alt="" data-align="center"><h2 id="swish" tabindex="-1"><a class="header-anchor" href="#swish" aria-hidden="true">#</a> Swish</h2><p>Swish函数是一种平滑的非线性函数，类似于SiLU函数，但不同之处在于它包含一个可调参数，可以控制函数的形状。</p><img src="'+u+'" title="" alt="" data-align="center"><h2 id="softplus" tabindex="-1"><a class="header-anchor" href="#softplus" aria-hidden="true">#</a> SoftPlus</h2><p>Softplus函数是Sigmoid函数的平滑版本，通常用于回归任务和一些概率模型。</p><img src="'+x+'" title="" alt="" data-align="center"><h2 id="hardtanh" tabindex="-1"><a class="header-anchor" href="#hardtanh" aria-hidden="true">#</a> Hardtanh</h2><p>Hardtanh函数是一个带有硬界限的激活函数，类似于Tanh，但在一定范围内输出线性函数。</p><img src="'+y+'" title="" alt="" data-align="center">',23),R=[L,b,_,U,v,S,k,w];function M(E,z){return n(),e("div",null,R)}const N=t(f,[["render",M],["__file","activation_function.html.vue"]]);export{N as default};
