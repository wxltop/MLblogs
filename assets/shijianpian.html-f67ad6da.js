const l=JSON.parse('{"key":"v-49b5d92b","path":"/blogs/damoxing/shijianpian.html","title":"大模型实践","lang":"en-US","frontmatter":{"title":"大模型实践","date":"2023/7/29","categories":["大模型"],"tags":["大模型"]},"headers":[{"level":2,"title":"大模型部署框架 FastLLM 实现细节解析","slug":"大模型部署框架-fastllm-实现细节解析","link":"#大模型部署框架-fastllm-实现细节解析","children":[]},{"level":2,"title":"使用LMDeploy部署Llama-2系列模型","slug":"使用lmdeploy部署llama-2系列模型","link":"#使用lmdeploy部署llama-2系列模型","children":[]},{"level":2,"title":"softmax对transformer的危害","slug":"softmax对transformer的危害","link":"#softmax对transformer的危害","children":[]},{"level":2,"title":"大模型的训练要不要用到pytorch的FSDP？","slug":"大模型的训练要不要用到pytorch的fsdp","link":"#大模型的训练要不要用到pytorch的fsdp","children":[]},{"level":2,"title":"大模型LLaMA及其Finetune方法","slug":"大模型llama及其finetune方法","link":"#大模型llama及其finetune方法","children":[]},{"level":2,"title":"一个开源方案，极速预训练650亿参数LLaMA","slug":"一个开源方案-极速预训练650亿参数llama","link":"#一个开源方案-极速预训练650亿参数llama","children":[]},{"level":2,"title":"ChatGLM2模型训练，少样本微调详细流程","slug":"chatglm2模型训练-少样本微调详细流程","link":"#chatglm2模型训练-少样本微调详细流程","children":[]},{"level":2,"title":"60分钟快速学习，ChatGLM2-6b训练微调应用案例","slug":"_60分钟快速学习-chatglm2-6b训练微调应用案例","link":"#_60分钟快速学习-chatglm2-6b训练微调应用案例","children":[]},{"level":2,"title":"【DeepSpeed 教程翻译】开始，安装细节和CIFAR-10 Tutorial","slug":"【deepspeed-教程翻译】开始-安装细节和cifar-10-tutorial","link":"#【deepspeed-教程翻译】开始-安装细节和cifar-10-tutorial","children":[]},{"level":2,"title":"大模型训练之微调篇","slug":"大模型训练之微调篇","link":"#大模型训练之微调篇","children":[]}],"git":{"createdTime":1690986485000,"updatedTime":1690986485000,"contributors":[{"name":"wxltop","email":"wxltop@163.com","commits":1}]},"filePathRelative":"blogs/大模型/实践篇.md"}');export{l as data};
