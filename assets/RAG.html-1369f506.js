import{_ as o,r as p,o as i,c as l,a as n,b as a,d as e,e as t}from"./app-aa9cafec.js";const c="/MLblogs/assets/2024-02-07-10-56-30-image-6369d2f5.png",r={},u={class:"custom-container info"},d=n("svg",{xmlns:"http://www.w3.org/2000/svg","xmlns:xlink":"http://www.w3.org/1999/xlink",viewBox:"0 0 24 24"},[n("g",{fill:"none",stroke:"currentColor","stroke-width":"2","stroke-linecap":"round","stroke-linejoin":"round"},[n("circle",{cx:"12",cy:"12",r:"9"}),n("path",{d:"M12 8h.01"}),n("path",{d:"M11 12h1v4h1"})])],-1),m=n("p",{class:"custom-container-title"},"INFO",-1),k={href:"https://mp.weixin.qq.com/s?__biz=MzkxNjYxMjUwMA==&mid=2247484519&idx=1&sn=55a9fa5107d3b69a019db36703b86538&chksm=c14c709cf63bf98a615255cb55041338eaaa024066f721c9c81456c99eaf8f423ae27a56fa8a&token=1160261652&lang=zh_CN#rd",target:"_blank",rel:"noopener noreferrer"},v=t('<h2 id="什么是rag" tabindex="-1"><a class="header-anchor" href="#什么是rag" aria-hidden="true">#</a> 什么是RAG?</h2><p>检索增强生成（Retrieval Augmented Generation），简称 RAG，已经成为当前最火热的LLM应用方案。经历今年年初那一波大模型潮，想必大家对大模型的能力有了一定的了解，但是当我们将大模型应用于实际业务场景时会发现，通用的基础大模型基本无法满足我们的实际业务需求，主要有以下几方面原因：</p><ul><li><p><strong>知识的局限性</strong>：模型自身的知识完全源于它的训练数据，而现有的主流大模型（ChatGPT、文心一言、通义千问…）的训练集基本都是构建于网络公开的数据，对于一些实时性的、非公开的或离线的数据是无法获取到的，这部分知识也就无从具备。</p></li><li><p><strong>幻觉问题</strong>：所有的AI模型的底层原理都是基于数学概率，其模型输出实质上是一系列数值运算，大模型也不例外，所以它有时候会一本正经地胡说八道，尤其是在大模型自身不具备某一方面的知识或不擅长的场景。而这种幻觉问题的区分是比较困难的，因为它要求使用者自身具备相应领域的知识。</p></li><li><p><strong>数据安全性</strong>：对于企业来说，数据安全至关重要，没有企业愿意承担数据泄露的风险，将自身的私域数据上传第三方平台进行训练。这也导致完全依赖通用大模型自身能力的应用方案不得不在数据安全和效果方面进行取舍。</p></li></ul><p>而RAG是解决上述问题的一套有效方案。</p><h3 id="rag架构" tabindex="-1"><a class="header-anchor" href="#rag架构" aria-hidden="true">#</a> RAG架构</h3><p>RAG的架构如图中所示，简单来讲，RAG就是<strong>通过检索获取相关的知识并将其融入Prompt，让大模型能够参考相应的知识从而给出合理回答</strong>。因此，可以将RAG的核心理解为“检索+生成”，前者主要是利用向量数据库的高效存储和检索能力，召回目标知识；后者则是利用大模型和Prompt工程，将召回的知识合理利用，生成目标答案。</p><img title="" src="'+c+'" alt="" data-align="center" width="480"><p>完整的RAG应用流程主要包含两个阶段：</p><ul><li><p>数据准备阶段：数据提取——&gt;文本分割——&gt;向量化（embedding）——&gt;数据入库</p></li><li><p>应用阶段：用户提问——&gt;数据检索（召回）——&gt;注入Prompt——&gt;LLM生成答案</p></li></ul><p>下面我们详细介绍一下各环节的技术细节和注意事项：</p><p><strong>数据准备阶段</strong>：</p><p>数据准备一般是一个离线的过程，主要是将私域数据向量化后构建索引并存入数据库的过程。主要包括：数据提取、文本分割、向量化、数据入库等环节。</p><ul><li><p><strong>数据提取</strong></p><p>数据加载：包括多格式数据加载、不同数据源获取等，根据数据自身情况，将数据处理为同一个范式。</p><p>数据处理：包括数据过滤、压缩、格式化等。</p><p>元数据获取：提取数据中关键信息，例如文件名、Title、时间等 。</p></li><li><p><strong>文本分割</strong>：</p><p>文本分割主要考虑两个因素：1）embedding模型的Tokens限制情况；2）语义完整性对整体的检索效果的影响。一些常见的文本分割方式如下：</p><blockquote><p>句分割：以”句”的粒度进行切分，保留一个句子的完整语义。常见切分符包括：句号、感叹号、问号、换行符等。</p><p>固定长度分割：根据embedding模型的token长度限制，将文本分割为固定长度（例如256/512个tokens），这种切分方式会损失很多语义信息，一般通过在头尾增加一定冗余量来缓解。</p></blockquote></li><li><p><strong>向量化（embedding）</strong>：</p><p>向量化是一个将文本数据转化为向量矩阵的过程，该过程会直接影响到后续检索的效果。目前常见的embedding模型如表中所示，这些embedding模型基本能满足大部分需求，但对于特殊场景（例如涉及一些罕见专有词或字等）或者想进一步优化效果，则可以选择开源Embedding模型微调或直接训练适合自己场景的Embedding模型。</p></li></ul>',13),g=n("thead",null,[n("tr",null,[n("th",null,"模型名称"),n("th",null,"描述"),n("th",null,"获取地址")])],-1),b=n("td",null,"ChatGPT-Embedding",-1),h=n("td",null,"ChatGPT-Embedding由OpenAI公司提供，以接口形式调用。",-1),_={href:"https://platform.openai.com/docs/guides/embeddings/what-are-embeddings",target:"_blank",rel:"noopener noreferrer"},y=n("td",null,"ERNIE-Embedding V1",-1),f=n("td",null,"ERNIE-Embedding V1由百度公司提供，依赖于文心大模型能力，以接口形式调用。",-1),w={href:"https://cloud.baidu.com/doc/WENXINWORKSHOP/s/alj562vvu",target:"_blank",rel:"noopener noreferrer"},q=n("td",null,"M3E",-1),x=n("td",null,"M3E是一款功能强大的开源Embedding模型，包含m3e-small、m3e-base、m3e-large等多个版本，支持微调和本地部署。",-1),C={href:"https://huggingface.co/moka-ai/m3e-base",target:"_blank",rel:"noopener noreferrer"},P=n("td",null,"BGE",-1),E=n("td",null,"BGE由北京智源人工智能研究院发布，同样是一款功能强大的开源Embedding模型，包含了支持中文和英文的多个版本，同样支持微调和本地部署。",-1),R={href:"https://huggingface.co/BAAI/bge-base-en-v1.5",target:"_blank",rel:"noopener noreferrer"},T=t(`<ul><li>数据入库：数据向量化后构建索引，并写入数据库的过程可以概述为数据入库过程，适用于RAG场景的数据库包括：FAISS、Chromadb、ES、milvus等。一般可以根据业务场景、硬件、性能需求等多因素综合考虑，选择合适的数据库。</li></ul><p><strong>应用阶段：</strong></p><p>在应用阶段，我们根据用户的提问，通过高效的检索方法，召回与提问最相关的知识，并融入Prompt；大模型参考当前提问和相关知识，生成相应的答案。关键环节包括：数据检索、注入Prompt等。</p><ul><li><p><strong>数据检索</strong></p><p>常见的数据检索方法包括：相似性检索、全文检索等，根据检索效果，一般可以选择多种检索方式融合，提升召回率。</p><ul><li><p>相似性检索：即计算查询向量与所有存储向量的相似性得分，返回得分高的记录。常见的相似性计算方法包括：余弦相似性、欧氏距离、曼哈顿距离等。</p></li><li><p>全文检索：全文检索是一种比较经典的检索方式，在数据存入时，通过关键词构建倒排索引；在检索时，通过关键词进行全文检索，找到对应的记录。</p></li></ul></li><li><p><strong>注入Prompt</strong></p><p>Prompt作为大模型的直接输入，是影响模型输出准确率的关键因素之一。在RAG场景中，Prompt一般包括<strong>任务描述、背景知识（检索得到）、任务指令</strong>（一般是用户提问）等，根据任务场景和大模型性能，也可以在Prompt中适当加入其他指令优化大模型的输出。一个简单知识问答场景的Prompt如下所示：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>【任务描述】假如你是一个专业的客服机器人，请参考【背景知识】，回
【背景知识】{content} // 数据检索得到的相关文本    
【问题】石头扫地机器人P10的续航时间是多久？
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>Prompt的设计只有方法、没有语法，比较依赖于个人经验，在实际应用过程中，往往需要根据大模型的实际输出进行针对性的Prompt调优。</p></li></ul><h2 id="实践案例——私域知识问答应用案例" tabindex="-1"><a class="header-anchor" href="#实践案例——私域知识问答应用案例" aria-hidden="true">#</a> 实践案例——私域知识问答应用案例</h2><p>本次选用百度百科——藜麦数据（https://baike.baidu.com/item/藜麦/5843874）模拟个人或企业私域数据，并基于langchain开发框架，实现一种简单的RAG问答应用示例。</p><p><strong>环境准备：</strong></p><div class="language-cmd line-numbers-mode" data-ext="cmd"><pre class="language-cmd"><code>pip install datasets langchain sentence_transformers tqdm chromadb langchain_wenxin
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p><strong>本地数据加载：</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>document_loaders <span class="token keyword">import</span> TextLoader
loader <span class="token operator">=</span> TextLoader<span class="token punctuation">(</span><span class="token string">&quot;./藜.txt&quot;</span><span class="token punctuation">)</span>
documents <span class="token operator">=</span> loader<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span>
documents
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>文档分割：</strong></p><p>文档分割，借助langchain的字符分割器，这里采用固定字符长度分割chunk_size=128</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 文档分割</span>
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>text_splitter <span class="token keyword">import</span> CharacterTextSplitter
<span class="token comment"># 创建拆分器</span>
text_splitter <span class="token operator">=</span> CharacterTextSplitter<span class="token punctuation">(</span>chunk_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span> chunk_overlap<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
<span class="token comment"># 拆分文档</span>
documents <span class="token operator">=</span> text_splitter<span class="token punctuation">.</span>split_documents<span class="token punctuation">(</span>documents<span class="token punctuation">)</span>
documents
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>向量化&amp;数据入库：</strong></p><p>接下来对分割后的数据进行embedding，并写入数据库。这里选用m3e-base作为embedding模型，向量数据库选用Chroma</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>embeddings <span class="token keyword">import</span> HuggingFaceBgeEmbeddings
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>vectorstores <span class="token keyword">import</span> Chroma

<span class="token comment"># embedding model: m3e-base</span>
model_name <span class="token operator">=</span> <span class="token string">&quot;moka-ai/m3e-base&quot;</span>
model_kwargs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&#39;device&#39;</span><span class="token punctuation">:</span> <span class="token string">&#39;cpu&#39;</span><span class="token punctuation">}</span>
encode_kwargs <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&#39;normalize_embeddings&#39;</span><span class="token punctuation">:</span> <span class="token boolean">True</span><span class="token punctuation">}</span>
embedding <span class="token operator">=</span> HuggingFaceBgeEmbeddings<span class="token punctuation">(</span>
                model_name<span class="token operator">=</span>model_name<span class="token punctuation">,</span>
                model_kwargs<span class="token operator">=</span>model_kwargs<span class="token punctuation">,</span>
                encode_kwargs<span class="token operator">=</span>encode_kwargs<span class="token punctuation">,</span>
                query_instruction<span class="token operator">=</span><span class="token string">&quot;为文本生成向量表示用于文本检索&quot;</span>
            <span class="token punctuation">)</span>

<span class="token comment"># load data to Chroma db</span>
db <span class="token operator">=</span> Chroma<span class="token punctuation">.</span>from_documents<span class="token punctuation">(</span>documents<span class="token punctuation">,</span> embedding<span class="token punctuation">)</span>
<span class="token comment"># similarity search</span>
db<span class="token punctuation">.</span>similarity_search<span class="token punctuation">(</span><span class="token string">&quot;藜一般在几月播种？&quot;</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>Prompt设计：</strong></p><p>prompt设计，这里只是一个prompt的简单示意，在实际业务场景中需要针对场景特点针对性调优。</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>template = &#39;&#39;&#39;
        【任务描述】
        请根据用户输入的上下文回答问题，并遵守回答要求。

        【背景知识】
        {{context}}

        【回答要求】
        - 你需要严格根据背景知识的内容回答，禁止根据常识和已知信息回答问题。
        - 对于不知道的信息，直接回答“未找到相关答案”
        -----------
        {question}
        &#39;&#39;&#39;
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>RetrievalqaChain构建：</strong></p><p>这里采用ConversationalRetrievalChain，ConversationalRetrievalQA chain 是建立在 RetrievalQAChain 之上，提供历史聊天记录组件。如下面定义了memory来追踪聊天记录，在流程上，先将历史问题和当前输入问题融合为一个新的独立问题，然后再进行检索，获取问题相关知识，最后将获取的知识和生成的新问题注入Prompt让大模型生成回答。</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> langchain <span class="token keyword">import</span> LLMChain
<span class="token keyword">from</span> langchain_wenxin<span class="token punctuation">.</span>llms <span class="token keyword">import</span> Wenxin
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>prompts <span class="token keyword">import</span> PromptTemplate
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationBufferMemory
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> ConversationalRetrievalChain
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>prompts<span class="token punctuation">.</span>chat <span class="token keyword">import</span> ChatPromptTemplate<span class="token punctuation">,</span> SystemMessagePromptTemplate<span class="token punctuation">,</span> HumanMessagePromptTemplate

<span class="token comment"># LLM选型</span>
llm <span class="token operator">=</span> Wenxin<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">&quot;ernie-bot&quot;</span><span class="token punctuation">,</span> baidu_api_key<span class="token operator">=</span><span class="token string">&quot;baidu_api_key&quot;</span><span class="token punctuation">,</span> baidu_secret_key<span class="token operator">=</span><span class="token string">&quot;baidu_secret_key&quot;</span><span class="token punctuation">)</span>

retriever <span class="token operator">=</span> db<span class="token punctuation">.</span>as_retriever<span class="token punctuation">(</span><span class="token punctuation">)</span>
memory <span class="token operator">=</span> ConversationBufferMemory<span class="token punctuation">(</span>memory_key<span class="token operator">=</span><span class="token string">&quot;chat_history&quot;</span><span class="token punctuation">,</span> return_messages<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
qa <span class="token operator">=</span> ConversationalRetrievalChain<span class="token punctuation">.</span>from_llm<span class="token punctuation">(</span>llm<span class="token punctuation">,</span> retriever<span class="token punctuation">,</span> memory<span class="token operator">=</span>memory<span class="token punctuation">)</span>
qa<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">&quot;question&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;藜怎么防治虫害？&quot;</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>高级用法：</strong></p><p>针对多轮对话场景，增加 question_generator对历史对话记录进行压缩生成新的question，增加combine_docs_chain对检索得到的文本进一步融合</p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">from</span> langchain <span class="token keyword">import</span> LLMChain
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>prompts <span class="token keyword">import</span> PromptTemplate
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationBufferMemory
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> ConversationalRetrievalChain<span class="token punctuation">,</span> StuffDocumentsChain
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains<span class="token punctuation">.</span>qa_with_sources <span class="token keyword">import</span> load_qa_with_sources_chain
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>prompts<span class="token punctuation">.</span>chat <span class="token keyword">import</span> ChatPromptTemplate<span class="token punctuation">,</span> SystemMessagePromptTemplate<span class="token punctuation">,</span> HumanMessagePromptTemplate


<span class="token comment"># 构建初始 messages 列表，这里可以理解为是 openai 传入的 messages 参数</span>
messages <span class="token operator">=</span> <span class="token punctuation">[</span>
  SystemMessagePromptTemplate<span class="token punctuation">.</span>from_template<span class="token punctuation">(</span>qa_template<span class="token punctuation">)</span><span class="token punctuation">,</span>
  HumanMessagePromptTemplate<span class="token punctuation">.</span>from_template<span class="token punctuation">(</span><span class="token string">&#39;{question}&#39;</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span>

<span class="token comment"># 初始化 prompt 对象</span>
prompt <span class="token operator">=</span> ChatPromptTemplate<span class="token punctuation">.</span>from_messages<span class="token punctuation">(</span>messages<span class="token punctuation">)</span>
llm_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>prompt<span class="token punctuation">)</span>

combine_docs_chain <span class="token operator">=</span> StuffDocumentsChain<span class="token punctuation">(</span>
    llm_chain<span class="token operator">=</span>llm_chain<span class="token punctuation">,</span>
    document_separator<span class="token operator">=</span><span class="token string">&quot;\\n\\n&quot;</span><span class="token punctuation">,</span>
    document_variable_name<span class="token operator">=</span><span class="token string">&quot;context&quot;</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
q_gen_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> prompt<span class="token operator">=</span>PromptTemplate<span class="token punctuation">.</span>from_template<span class="token punctuation">(</span>qa_condense_template<span class="token punctuation">)</span><span class="token punctuation">)</span>

qa <span class="token operator">=</span> ConversationalRetrievalChain<span class="token punctuation">(</span>combine_docs_chain<span class="token operator">=</span>combine_docs_chain<span class="token punctuation">,</span>
                                  question_generator<span class="token operator">=</span>q_gen_chain<span class="token punctuation">,</span>
                                  return_source_documents<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                  return_generated_question<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                  retriever<span class="token operator">=</span>retriever<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>qa<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">&#39;question&#39;</span><span class="token punctuation">:</span> <span class="token string">&quot;藜麦怎么防治虫害？&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;chat_history&quot;</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,25);function M(A,G){const s=p("ExternalLinkIcon");return i(),l("div",null,[n("div",u,[d,m,n("p",null,[n("a",k,[a("一文搞懂大模型RAG应用（附实践案例） (qq.com)"),e(s)])])]),v,n("table",null,[g,n("tbody",null,[n("tr",null,[b,h,n("td",null,[n("a",_,[a("ChatGPT-Embedding"),e(s)])])]),n("tr",null,[y,f,n("td",null,[n("a",w,[a("ERNIE-Embedding"),e(s)])])]),n("tr",null,[q,x,n("td",null,[n("a",C,[a("M3E"),e(s)])])]),n("tr",null,[P,E,n("td",null,[n("a",R,[a("BGE"),e(s)])])])])]),T])}const B=o(r,[["render",M],["__file","RAG.html.vue"]]);export{B as default};
