import{_ as r,r as s,o as i,c as n,a,b as e,d as h,e as d}from"./app-aa9cafec.js";const c="/MLblogs/imgs/2023-07-29-17-34-18-image.png",o="/MLblogs/imgs/2023-07-29-17-36-16-image.png",p={},m=a("h2",{id:"大模型部署框架-fastllm-实现细节解析",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#大模型部署框架-fastllm-实现细节解析","aria-hidden":"true"},"#"),e(" 大模型部署框架 FastLLM 实现细节解析")],-1),l=a("p",null,"https://mp.weixin.qq.com/s/o4MhrrOyTEmBxNBH5tu2zA",-1),f=a("h2",{id:"使用lmdeploy部署llama-2系列模型",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#使用lmdeploy部署llama-2系列模型","aria-hidden":"true"},"#"),e(" 使用LMDeploy部署Llama-2系列模型")],-1),x={href:"https://mp.weixin.qq.com/s?__biz=MzI4MDcxNTY2MQ==&mid=2247516919&idx=1&sn=cf68273e4dc4fffd586ddff6554b2b7f&chksm=ebb68a15dcc10303367ac9622ea926c7763e4bae78423550a60b109f5da96fe2adc66a574afd&mpshare=1&scene=1&srcid=0727BksKi2d9WbsSL4C6HS1i&sharer_sharetime=1690435917679&sharer_shareid=f48d477bdc57653f558f1018c59d93fe#rd",target:"_blank",rel:"noopener noreferrer"},_=d('<h2 id="softmax对transformer的危害" tabindex="-1"><a class="header-anchor" href="#softmax对transformer的危害" aria-hidden="true">#</a> softmax对transformer的危害</h2><p>https://mp.weixin.qq.com/s/OKWL8eGjQIbIGI4kc4r1Dg</p><p>要部署大模型就得压缩，但是有一些异常表示会一直保留一个低概率分布值，无法压缩。在一些离散表示中，softmax都显得有效，若不想保留某些项的时候必须对softmax修改，否则会产生扭曲。比如在LLM中，扭曲产生的原因是对非对称语义token（如逗号等）。</p><div align="center"><img src="'+c+'"></div><p>Vanilla softmax 将始终释出相同的总权重；softmax_1 看起来大部分相同，但在负象限中有一个「逃出口」（escape hatch）。需要明确的是，这里的核心问题在本质上是数学而非数值问题。额外的精度并不能拯救 softmax，所有的 Transformers 都会受到影响。</p><p>你还可以观察到关于 softmax_1 的其他一些事项。导数是正的，所以总是有一个非零梯度，并且它的和介于 0 和 1 之间，所以输出不会失控。该函数保持以下属性:</p><div align="center"><img src="'+o+'"></div><p>即输出向量中的相对值不变。</p><h2 id="大模型的训练要不要用到pytorch的fsdp" tabindex="-1"><a class="header-anchor" href="#大模型的训练要不要用到pytorch的fsdp" aria-hidden="true">#</a> 大模型的训练要不要用到pytorch的FSDP？</h2><p>https://mp.weixin.qq.com/s/PyhS95_fJMrRi_PncKu3Pw</p><h2 id="大模型llama及其finetune方法" tabindex="-1"><a class="header-anchor" href="#大模型llama及其finetune方法" aria-hidden="true">#</a> 大模型LLaMA及其Finetune方法</h2><p>https://mp.weixin.qq.com/s/3QdqMANvGmVY7ftf1c-hyw</p><h2 id="一个开源方案-极速预训练650亿参数llama" tabindex="-1"><a class="header-anchor" href="#一个开源方案-极速预训练650亿参数llama" aria-hidden="true">#</a> 一个开源方案，极速预训练650亿参数LLaMA</h2><p>https://mp.weixin.qq.com/s/NG-SNAGLvNeYx33tVkkWQA</p><h2 id="chatglm2模型训练-少样本微调详细流程" tabindex="-1"><a class="header-anchor" href="#chatglm2模型训练-少样本微调详细流程" aria-hidden="true">#</a> ChatGLM2模型训练，少样本微调详细流程</h2><p>https://mp.weixin.qq.com/s/CGgEj01joWZMkfqmIKRfdQ</p><h2 id="_60分钟快速学习-chatglm2-6b训练微调应用案例" tabindex="-1"><a class="header-anchor" href="#_60分钟快速学习-chatglm2-6b训练微调应用案例" aria-hidden="true">#</a> 60分钟快速学习，ChatGLM2-6b训练微调应用案例</h2><p>https://mp.weixin.qq.com/s/JtnrgX1VrEYjJRWrxWtKaw</p><h2 id="【deepspeed-教程翻译】开始-安装细节和cifar-10-tutorial" tabindex="-1"><a class="header-anchor" href="#【deepspeed-教程翻译】开始-安装细节和cifar-10-tutorial" aria-hidden="true">#</a> 【DeepSpeed 教程翻译】开始，安装细节和CIFAR-10 Tutorial</h2><p>https://mp.weixin.qq.com/s/xpNQtl7hPs3fy9S7VRbIkg</p><h2 id="大模型训练之微调篇" tabindex="-1"><a class="header-anchor" href="#大模型训练之微调篇" aria-hidden="true">#</a> 大模型训练之微调篇</h2><p>https://mp.weixin.qq.com/s/__MqmNP40PzcG1sRKROoXA</p>',22);function b(q,u){const t=s("ExternalLinkIcon");return i(),n("div",null,[m,l,f,a("p",null,[a("a",x,[e("使用 LMDeploy 轻松部署 Llama-2 系列模型！ (qq.com)"),h(t)])]),_])}const L=r(p,[["render",b],["__file","shijianpian.html.vue"]]);export{L as default};
