import{_ as n}from"./2024-03-19-10-25-41-image-a8040aff.js";import{_ as i,r as m,o as r,c as p,a as s,b as a,d as l,e as t}from"./app-aa9cafec.js";const c="/MLblogs/assets/2024-04-05-23-48-50-image-54be2152.png",o="/MLblogs/assets/2024-04-06-09-07-52-image-fd36c8fd.png",h="/MLblogs/assets/2024-04-06-11-26-14-image-8fb40736.png",g="/MLblogs/assets/2024-04-06-11-50-48-image-b4e207e8.png",d="/MLblogs/assets/2024-04-06-12-58-02-image-e343c129.png",u="/MLblogs/assets/2024-04-06-13-01-40-image-61c272fb.png",x="/MLblogs/assets/2024-04-06-13-01-53-image-c2f293a4.png",b="/MLblogs/assets/2024-04-05-14-13-01-image-817b34cd.png",w="/MLblogs/assets/2024-04-05-14-07-12-image-1c2e1897.png",v="/MLblogs/assets/2024-04-05-16-47-49-image-890107a2.png",y="/MLblogs/assets/2024-04-05-15-31-48-image-e633e8d8.png",_={},k={class:"custom-container info"},f=s("svg",{xmlns:"http://www.w3.org/2000/svg","xmlns:xlink":"http://www.w3.org/1999/xlink",viewBox:"0 0 24 24"},[s("g",{fill:"none",stroke:"currentColor","stroke-width":"2","stroke-linecap":"round","stroke-linejoin":"round"},[s("circle",{cx:"12",cy:"12",r:"9"}),s("path",{d:"M12 8h.01"}),s("path",{d:"M11 12h1v4h1"})])],-1),N=s("p",{class:"custom-container-title"},"INFO",-1),M={href:"https://github.com/datawhalechina/fun-rec/blob/master/docs/ch02/ch2.1/ch2.1.2/YoutubeDNN.md",target:"_blank",rel:"noopener noreferrer"},z=t('<p>推荐系统的链路是：首先将用户过去的点击历史、用户id、性别、年龄、物品特征等作为特征输入召回网络，召回层会从亿级别的物品中选出千级别的物品，传给排序层。召回要尽可能召回更加丰富的内容，速度尽可能快。排序分为粗排和精排，粗排从千级别的物品选出百级别的物品，并打分，传给精排模型，精排给百级别的物品打分传给重排。重排结合多样性采样，选出几十个物品曝光给用户。</p><p>召回通常由多个召回通路组成，每一个召回通路可以看成一种召回策略，常见召回策略有：基于用户兴趣标签的召回；基于协同过滤的召回；基于热点的召回；基于地域的召回；基于Topic的召回；基于命名实体的召回等等。</p><h2 id="双塔模型" tabindex="-1"><a class="header-anchor" href="#双塔模型" aria-hidden="true">#</a> 双塔模型</h2><p>从模型结构上来看，主要包括两个部分：user侧塔和item侧塔，对于每个塔分别是一个DNN结构。通过两侧的特征输入，通过DNN模块到user和item的embedding，然后计算两者之间的相似度:</p><img title="" src="'+c+'" alt="" data-align="center" width="262"><p>之所以在实际应用中非常常见，是因为<strong>在海量的候选数据进行召回的场景下，速度很快，效果说不上极端好，但一般而言效果也够用了</strong>。之所以双塔模型在服务时速度很快，是因为模型结构简单(两侧没有特征交叉)，但这也带来了问题，双塔的结构无法考虑两侧特征之间的交互信息，<strong>在一定程度上牺牲掉模型的部分精准性</strong>。</p><h3 id="senet双塔模型" tabindex="-1"><a class="header-anchor" href="#senet双塔模型" aria-hidden="true">#</a> SENet双塔模型</h3><p>https://zhuanlan.zhihu.com/p/80123284</p><p>SENet用在排序模型中，主要是为了<strong>将大量长尾的低频特征域抛弃，弱化不靠谱低频特征embedding的负面影响，强化高频特征的重要作用</strong>。</p><h4 id="senet介绍" tabindex="-1"><a class="header-anchor" href="#senet介绍" aria-hidden="true">#</a> SENet介绍</h4><p>SENet(Squeeze-and-Excitation Networks)，于图像领域提出，<strong>通过学习的方式来自动获取到每个特征通道的重要程度</strong>，然后依照这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征。</p><p>给定一个输入 x，其特征通道数为 c_1，通过一系列卷积等一般变换后得到一个特征通道数为 c_2 的特征。接下来做三个操作：</p>',12),E=s("ul",null,[s("li",null,[s("p",null,[a("首先是 Squeeze 操作，我们顺着空间维度来进行特征压缩，将每个二维的特征通道变成一个实数，这个实数某种程度上具有全局的感受野，并且输出的维度和输入的特征通道数相匹配。它表征着在特征通道上响应的全局分布，而且使得靠近输入的层也可以获得全局的感受野，这一点在很多任务中都是非常有用的。"),s("strong",null,[a("对应"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"F"),s("mrow",null,[s("mi",null,"s"),s("mi",null,"q")])])]),s("annotation",{encoding:"application/x-tex"},"F_{sq}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.9694em","vertical-align":"-0.2861em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"F"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"s"),s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.03588em"}},"q")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])])])])]),a("操作，实际实现的时候使用一个Global Avg Pooling")]),a("。")])]),s("li",null,[s("p",null,[a("其次是 Excitation 操作，它是一个类似于循环神经网络中门的机制。通过参数 w 来为每个特征通道生成权重，其中参数 w 被学习用来显式地建模特征通道间的相关性。"),s("strong",null,[a("对应"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"F"),s("mrow",null,[s("mi",null,"e"),s("mi",null,"x")])])]),s("annotation",{encoding:"application/x-tex"},"F_{ex}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"F"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.1389em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"e"),s("span",{class:"mord mathnormal mtight"},"x")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("操作，实际实现的时候，使用两个全连接层：一个W1是"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"C"),s("mn",null,"2"),s("mo",null,"×"),s("mfrac",null,[s("mrow",null,[s("mi",null,"C"),s("mn",null,"2")]),s("mi",null,"r")])]),s("annotation",{encoding:"application/x-tex"},"C2\\times \\frac{C2}{r}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.7667em","vertical-align":"-0.0833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"C"),s("span",{class:"mord"},"2"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.2173em","vertical-align":"-0.345em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8723em"}},[s("span",{style:{top:"-2.655em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"r")])])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.394em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.07153em"}},"C"),s("span",{class:"mord mtight"},"2")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.345em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})])])])]),a("，另一个W2是"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mfrac",null,[s("mrow",null,[s("mi",null,"C"),s("mn",null,"2")]),s("mi",null,"r")]),s("mo",null,"×"),s("mi",null,"C"),s("mn",null,"2")]),s("annotation",{encoding:"application/x-tex"},"\\frac{C2}{r}\\times C2")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.2173em","vertical-align":"-0.345em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8723em"}},[s("span",{style:{top:"-2.655em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02778em"}},"r")])])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.394em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.07153em"}},"C"),s("span",{class:"mord mtight"},"2")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.345em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"C"),s("span",{class:"mord"},"2")])])]),a("，最后在使用sigmoid层")])])]),s("li",null,[s("p",null,[a("最后是一个 Reweight 的操作，我们将 Excitation 的输出的权重看做是经过特征选择后的每个特征通道的重要性，然后通过乘法逐通道加权到先前的特征上，完成在通道维度上的对原始特征的重标定。"),s("strong",null,"实际实现的时候就是将1x1xc2的向量与wxhxc2的张量相乘"),a("。")])])],-1),L=s("img",{src:o,title:"",alt:"","data-align":"center"},null,-1),S=s("h4",{id:"senet用于推荐",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#senet用于推荐","aria-hidden":"true"},"#"),a(" SENet用于推荐")],-1),D={href:"https://zhuanlan.zhihu.com/p/358779957",target:"_blank",rel:"noopener noreferrer"},C=t('<blockquote><p>推荐领域里面的特征有个特点，就是海量稀疏，意思是大量长尾特征是低频的，而这些低频特征，去学一个靠谱的Embedding是基本没希望的，但是你又不能把低频的特征全抛掉，因为有一些又是有效的。既然这样，如果我们把SENet用在特征Embedding上，类似于做了个对特征的Attention，弱化那些不靠谱低频特征Embedding的负面影响，强化靠谱低频特征以及重要中高频特征的作用</p></blockquote><p>在推荐系统中，通过SENet网络，动态地学习这些特征的重要性，对于每个特征学会一个特征权重，然后再把学习到的权重乘到对应特征的Embedding里，这样就可以动态学习特征权重，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的。</p><img src="'+h+'" title="" alt="" data-align="center"><p><strong>为何有效？</strong></p><blockquote><p>双塔模型的问题在于User侧特征和Item侧特征<strong>交互太晚</strong>，在高层交互，会造成细节信息，也就是<strong>具体特征信息的损失</strong>，影响两侧特征交叉的效果。而SENet模块在最底层就进行了<strong>特征的过滤</strong>，使得很多<strong>无效低频特征即使被过滤掉</strong>，这样更多有用的信息被保留到了双塔的最高层，使得两侧的交叉效果很好；同时由于<strong>SENet模块选择出更加重要的信息，使得User侧和Item侧特征之间的交互表达方面增强了DNN双塔的能力。</strong></p><p>因此SENet双塔模型主要是从<strong>特征选择的角度，提高了两侧特征交叉的有效性，减少了噪音对有效信息的干扰</strong>，进而提高了双塔模型的效果。</p></blockquote><h4 id="多兴趣senet双塔模型" tabindex="-1"><a class="header-anchor" href="#多兴趣senet双塔模型" aria-hidden="true">#</a> 多兴趣SENet双塔模型</h4><p>其实还可以进一步改进，目前在深度排序或者召回技术方面，一个趋势是把用户的单兴趣Embedding拓展到多兴趣Embedding，这样可以更细致地表达用户兴趣。</p><p>类似的，我们其实可以基于SENet，把User侧和Item侧的Embedding，打成多兴趣的。就是说，比如在用户侧塔，可以配置不同的SENet模块及对应的DNN结构，来强化不同方面兴趣的Embedding表达。Item侧也可以如此办理，或者Item侧如果信息比较单一，可以仍然只打出一个Item Embedding</p><img src="'+g+'" title="" alt="" data-align="center"><h4 id="多目标的双塔模型" tabindex="-1"><a class="header-anchor" href="#多目标的双塔模型" aria-hidden="true">#</a> 多目标的双塔模型</h4><p>现如今多任务学习在实际的应用场景也十分的常见，主要是因为实际场景中业务复杂，往往有很多的衡量指标，例如点击，评论，收藏，关注，转发等。在多任务学习中，往往会针对不同的任务使用一个独有的tower，然后优化不同任务损失.</p><img src="'+d+'" title="" alt="" data-align="center"><p>在user侧和item侧分别通过多个通道(DNN结构)为每个任务得到一个user embedding和item embedding，然后针对不同的目标分别计算user 和 item 的相似度，并计算各个目标的损失，最后的优化目标可以是多个任务损失之和，或者使用多任务学习中的动态损失权重。</p><h4 id="细节" tabindex="-1"><a class="header-anchor" href="#细节" aria-hidden="true">#</a> 细节</h4>',14),q={href:"https://dl.acm.org/doi/pdf/10.1145/3298689.3346996",target:"_blank",rel:"noopener noreferrer"},G=t('<ul><li><p>归一化：对user侧和item侧的输入embedding，进行L2归一化：</p><img src="'+u+'" title="" alt="" data-align="center"><img src="'+x+'" title="" alt="" data-align="center"><p>归一化的操作主要原因是因为向量点积距离是非度量空间，不满足三角不等式，而<strong>归一化的操作使得点击行为转化成了欧式距离</strong>。那为啥非要转为欧式距离呢？这是因为ANN一般是通过计算欧式距离进行检索，这样转化成欧式空间，保证训练和检索一致。</p></li><li><p>温度系数：在归一化之后的向量计算內积之后，除以一个固定的超参 r ，论文中命名为温度系数。</p></li></ul><h2 id="youtubednn" tabindex="-1"><a class="header-anchor" href="#youtubednn" aria-hidden="true">#</a> YouTubeDNN</h2><p>用户在某一个时刻点击了某个视频， 可以建模成输入一个用户向量， 从海量视频中预测出被点击的那个视频的概率。时刻t下， 用户U在背景C下对每个视频i的观看行为建模成下面的公式：</p><img src="'+b+'" title="" alt="" data-align="center"><p>u表示用户向量， 这里的v表示视频向量， 两者的维度都是N。分母是用户向量u与所有视频v的相似程度求和。</p><h3 id="和双塔模型的关系" tabindex="-1"><a class="header-anchor" href="#和双塔模型的关系" aria-hidden="true">#</a> 和双塔模型的关系</h3><p>YouTubeDNN模型虽然叫单塔模型，但也是以双塔模型的思想去构建的。YouTubeDNN召回模型结构如下，模型最终的输出就是用户的向量，然后用户向量和物品向量计算相似度，所以这里还有一个物品向量矩阵，图中没有画出来，原论文说物品的embedding通过word2vec的方式事先算好，直接作为了输入：</p><img src="'+w+'" title="" alt="" data-align="right"><p>所以容易看出来YouTubeDNN其实也是一个双塔模型结构，主要训练的部分是用户塔。</p><h3 id="输入" tabindex="-1"><a class="header-anchor" href="#输入" aria-hidden="true">#</a> 输入</h3><p>输入主要是用户侧的特征，包括用户观看的历史video序列， 用户搜索的历史tokens， 然后就是用户的人文特征，比如地理位置， 性别，年龄等。</p>',11),V=s("ul",null,[s("li",null,[s("p",null,[a("用户历史序列，历史搜索tokens这种序列性的特征: 一般长这样"),s("code",null,"[item_id5, item_id2, item_id3, ...]"),a("， 这种id特征是高维稀疏，首先会通过一个embedding层，转成低维稠密的embedding特征，即历史序列里面的每个id都会对应一个embedding向量， 这样历史序列就变成了多个embedding向量的形式， 这些向量一般会进行融合，常见的是average pooling，即每一维求平均得到一个最终向量来表示用户的历史兴趣或搜索兴趣。")]),s("blockquote",null,[s("p",null,"这里值的一提的是这里的embedding向量得到的方式， 论文中作者这里说是通过word2vec方法计算的， 关于word2vec，这里就不过多解释，也就是每个item事先通过w2v方式算好了的embedding，直接作为了输入，然后进行pooling融合。"),s("p",null,"除了这种算好embedding方式之外，还可以过embedding层，跟上面的DNN一起训练，这些都是常规操作，之前整理的精排模型里面大都是用这种方式。")]),s("p",null,"论文里面使用了用户最近的50次观看历史，用户最近50次搜索历史token， embedding维度是256维， 采用的average pooling。 当然，这里还可以把item的类别信息也隐射到embedding， 与前面的concat起来。")]),s("li",null,[s("p",null,[a("用户人文特征， 这种特征处理方式就是离散型的依然是labelEncoder，然后embedding转成低维稠密。 而"),s("strong",null,"连续型特征"),a("，一般是"),s("strong",null,"先归一化操作，然后直接输入"),a("，当然有的也通过分桶，转成离散特征，这里不过多整理，特征工程做的事情了。 当然，这里还有一波操作值得注意，就是"),s("strong",null,[a("连续型特征除了用了"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"x")]),s("annotation",{encoding:"application/x-tex"},"x")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal"},"x")])])]),a("本身，还用了"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"x"),s("mn",null,"2")])]),s("annotation",{encoding:"application/x-tex"},"x^2")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8141em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"x"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8141em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])])])])])])])])]),a("，"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"l"),s("mi",null,"o"),s("mi",null,"g"),s("mi",null,"x")]),s("annotation",{encoding:"application/x-tex"},"logx")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8889em","vertical-align":"-0.1944em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),s("span",{class:"mord mathnormal"},"o"),s("span",{class:"mord mathnormal"},"gx")])])]),a("这种， 可以加入更多非线性，增加模型表达能力。")]),s("br"),a(" 这些特征对新用户的推荐会比较有帮助，常见的用户的地理位置， 设备， 性别，年龄等。")])]),s("li",null,[s("p",null,[a("这里一个比较特色的特征是"),s("strong",null,"example age"),a("，这个特征后面需要单独整理。")])])],-1),I=t('<p>这些特征处理好了之后，拼接起来，就成了一个非常长的向量，然后就是过DNN，这里用了一个三层的DNN， 得到了输出， 这个输出也是向量。</p><h3 id="训练youtubednn" tabindex="-1"><a class="header-anchor" href="#训练youtubednn" aria-hidden="true">#</a> 训练YouTubeDNN</h3><p>在得到用户向量和物品向量之后，一个做法是计算该用户向量与所有物品向量的关联度，再经过softmax，但是这样计算量相当大，所以参考了word2vec的做法，采样负样本，把多分类问题转成了多个二分类的问题。 也就是不用全部的视频，而是随机选择出了一些没点的视频， 标记为0， 点了的视频标记为1， 这样就成了二分类的问题。</p><h4 id="回忆word2vec" tabindex="-1"><a class="header-anchor" href="#回忆word2vec" aria-hidden="true">#</a> 回忆word2vec</h4><p><strong>word2vec核心思想</strong>是：一个单词的含义由其上下文赋予。word2vec有两种模型结构，即skip-gram和CBOW。前者给定一个单词，预测上下文单词在其周围出现的概率；后者给定上下文，预测一个单词的概率。P(o∣c)表示“给定中心词c,它的上下文词o在它周围出现了：</p><img src="'+v+'" title="" alt="" data-align="center">',6),T=s("p",null,[a("注：注意到上图，中心词词向量为"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"v"),s("mi",null,"c")])]),s("annotation",{encoding:"application/x-tex"},"v_c")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"v"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"c")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("​,而上下文词词向量为"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"u"),s("mi",null,"o")])]),s("annotation",{encoding:"application/x-tex"},"u_o")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"u"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"o")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("。也就是说每个词会对应两个词向量，"),s("strong",null,[a("在词w做中心词时，使用"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"v"),s("mi",null,"w")])]),s("annotation",{encoding:"application/x-tex"},"v_w")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"v"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02691em"}},"w")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("​作为词向量，而在它做上下文词时，使用"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"u"),s("mi",null,"w")])]),s("annotation",{encoding:"application/x-tex"},"u_w")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"u"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02691em"}},"w")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("​作为词向量")]),a("，这样做的原因是为了求导等操作时计算上的简便。当整个模型训练完成后，我们既可以使用"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"v"),s("mi",null,"w")])]),s("annotation",{encoding:"application/x-tex"},"v_w")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"v"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02691em"}},"w")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("作为词w的词向量，也可以使用"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"u"),s("mi",null,"w")])]),s("annotation",{encoding:"application/x-tex"},"u_w")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.5806em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"u"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.1514em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.02691em"}},"w")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("作为词w的词向量，亦或是将二者平均。")],-1),W=s("p",null,[s("strong",null,"构造样本"),a("：从互联网爬下来一个文段，用一个滑动窗口，假设窗口大小为5，下图蓝色是输入单词，需要预测窗口内其他单词的概率，这个概率值表示的含义可以是蓝色单词与该单词在上下文中的关联度大小。")],-1),Y=s("img",{src:n,title:"",alt:"","data-align":"center"},null,-1),B=s("p",null,[s("strong",null,"训练步骤"),a("：假设单词量为V，embedding长度为d，skip-gram由两部分可训练参数组成：一个Vxd的权重W和一个dxV的权重W'，每一个单词构造成一个1xV的one-hot向量，输入网络，与W相乘（相当于选出一个维度为d的embedding）得到1xd的向量"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"v")]),s("annotation",{encoding:"application/x-tex"},"v")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.4306em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"v")])])]),a("，然后与W'相乘，得到一个1xV的向量，执行softmax，得到每一个单词作为输入单词的上下文的概率。训练好了之后抛弃W'，使用W，每当传进一个单词的one-hot向量就可以得到该单词的1xd的embedding。")],-1),F=t('<p><strong>负样本</strong>：但是上面过程中，直接对1xV输出向量执行softmax计算代价很大，特别是当V很大的时候。所以作者一个改进的做法是，构造一个正样本，少量负样本。负样本中另一个单词可以是其他与输入单词不相关的单词：</p><img src="'+y+'" title="" alt="" data-align="center"><p>这样做就可以将一个多分类问题（对1xV向量执行softmax）转换为二分类问题（仅仅使得输入两个单词的相似度接近1或接近0）。</p><h3 id="启发" tabindex="-1"><a class="header-anchor" href="#启发" aria-hidden="true">#</a> 启发</h3><p>YouTubeDNN论文中有很多工业界非常实用的经验，这里总结一下。</p><ol><li><p><strong>训练数据中对于每个用户选取相同的样本数， 保证用户在损失函数等权重</strong>， 因为这样可以减少高度活跃用户对于loss的影响。可以改进线上A/B测试的效果。</p></li><li><p>在生成样本的时候， 如果我们的用户比较少，行为比较少， 是不足以训练一个较好的召回模型，此时一个用户的历史观看序列，可以采用滑动窗口的形式生成多个训练样本， 比如一个用户的历史观看记录是&quot;abcdef&quot;， 那么采用滑动窗口， 可以是abc预测d, bcd预测e, cde预测f，这样一个用户就能生成3条训练样本。</p><p>原来的做法是它前后的用户行为都可以用来产生特征行为输入(word2vec的CBOW做样本的方法)。 而作者担心这一点会导致信息泄露， 模型<strong>不该知道的信息是未来的用户行为</strong>， 所以作者的做法是只使用更早时间的用户行为来产生特征， 这个也是目前通用的做法。</p></li><li><p>Example Age</p></li><li><p>DNN的结构对推荐效果的影响，对于DNN的层级，作者尝试了0~4层， 实验结果是<strong>层数越多越好， 但4层之后提升很有限， 层数越多训练越困难</strong></p></li><li><p>从&quot;双塔&quot;的角度再看YouTubeDNN召回模型， 这里的DNN个结构，其实就是一个用户塔， 输入用户的特征，最终通过DNN，编码出了用户的embedding向量。</p></li><li><p><strong>线上服务的时候</strong>， YouTube<strong>采用了一种最近邻搜索的方法去完成topK推荐</strong>，这其实是工程与学术trade-off的结果， model serving过程中对几百万个候选集一一跑模型显然不现实， 所以通过召回模型得到用户和video的embedding之后， 用最近邻搜索的效率会快很多。</p></li><li><p>训练数据的样本来源应该是全部物料， 而不仅仅是被推荐的物料，否则对于新物料难以曝光</p></li><li><p>序列无序化: 用户的最近一次搜索与搜索之后的播放行为有很强关联，为了避免信息泄露，将搜索行为顺序打乱。</p></li><li><p>训练数据构造: 预测接下来播放而不是用传统cbow中的两侧预测中间的考虑是可以防止信息泄露</p></li></ol><h2 id="基于图的召回" tabindex="-1"><a class="header-anchor" href="#基于图的召回" aria-hidden="true">#</a> 基于图的召回</h2><h3 id="eges" tabindex="-1"><a class="header-anchor" href="#eges" aria-hidden="true">#</a> EGES</h3><p>EGES模型主要是解决了匹配阶段的问题，通过用户行为计算商品间两两的相似性，然后根基相似性选出topK的商品输入到排序阶段。</p><p>为了学习更好的商品向量表示，本文通过用户的行为历史中构造一个item-item 图，然后应用随机游走方法在item-item 图为每个item获取到一个序列，然后通过Skip-Gram的方式为每个item学习embedding(这里的item序列类似于语句，其中每个item类比于句子中每个word)，这种方式被称为图嵌入方法(Graph Embedding)。</p><p>考虑可扩展性的问题，图嵌入的随机游走方式可以在物品图上捕获<strong>物品之间高阶相似性</strong>，即Base Graph Embedding（BGE）方法。其不同于CF方法，除了考虑物品的共现，还考虑到了行为的序列信息。</p><p>考虑到稀疏性和冷启物品问题，在图嵌入的基础上，考虑了节点的属性信息。希望具有相似属性的物品可以在空间上相似，即希望通过头部物品，提高属性信息的泛化能力，进而帮助尾部和冷启物品获取更加准确的embedding，即Graph Embedding with Side Information（GES）方法。</p><p>考虑到不同属性信息对于学习embedding的贡献不同，因此在聚合不同的属性信息时，动态的学习不同属性对于学习节点的embedding所参与的重要性权重，即Enhanced Graph Embedding with Side Information（EGES）。</p>',13);function A(U,O){const e=m("ExternalLinkIcon");return r(),p("div",null,[s("div",k,[f,N,s("p",null,[s("a",M,[a("fun-rec/docs/ch02/ch2.1/ch2.1.2/YoutubeDNN.md at master · datawhalechina/fun-rec (github.com)"),l(e)])])]),z,E,L,S,s("p",null,[s("a",D,[a("SENet双塔模型：在推荐领域召回粗排的应用及其它 - 知乎 (zhihu.com)"),l(e)])]),C,s("p",null,[a("在"),s("a",q,[a("Google的双塔召回模型"),l(e)]),a("中，重点介绍了两个trick，将user和item侧输出的embedding进行归一化以及对于內积值除以温度系数，实验证明这两种方式可以取得十分好的效果。")]),G,V,I,T,W,Y,B,F])}const j=i(_,[["render",A],["__file","recall.html.vue"]]);export{j as default};
