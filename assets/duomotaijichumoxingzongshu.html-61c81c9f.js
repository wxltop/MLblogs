import{_ as a,r as n,o as r,c as s,a as e,b as t,d as l,e as c}from"./app-aa9cafec.js";const i="/MLblogs/imgs/2023-11-12-15-50-23-image.png",p={},d={href:"http://arxiv.org/abs/2309.10020",target:"_blank",rel:"noopener noreferrer"},m=c('<p>文章是微软公司的几位华人联合组织的项目，介绍了展示视觉和视觉语言能力的多模态基础模型的分类和进化的全面调查，专注于从专业模型到通用助理的过渡。研究领域包括五个核心主题，分为两类：（1）首先调查了一些成熟的研究领域:针对特定目的预先训练的多模态基础模型，包括两个主题:用于视觉理解的视觉主干学习方法和文本到图像生成方法；（2）然后，介绍了探索性开放研究领域的最新进展:旨在发挥通用助手作用的多模态基础模型，包括三个主题-受大型语言模型(llm)启发的统一视觉模型，多模态llm的端到端训练，以及与llm链接的多模态工具。本文的目标受众是渴望了解多模态基础模型的基础知识和最新进展的计算机视觉和视觉语言多模态社区的研究人员、研究生和专业人员。</p><p>NLP，CV，多模态模型的进化有下图类似的趋势（<strong>从针对特定用途的预训练模型，到统一模型和通用助手</strong>），以NLP为例：（1）在早期，任务特定模型是为单个数据集和任务开发的，通常是从头开始训练的；（2）通过大规模的预训练，语言模型在许多既定的语言理解和生成任务上达到了最先进的性能，如BERT，RoBERTa，T5，DeBERTa，GPT-2。这些预训练模型为下游任务适应提供了基础。（3）以GPT-3 为例，大型语言模型(llm)将各种语言理解和生成任务统一到一个模型中。随着网络规模的培训和统一，一些新兴的能力出现了，比如情境学习和思维链。（4）随着人类与人工智能结合的最新进展，LLM开，始扮演通用助手的角色，遵循人类的意图，完成各种语言任务。</p><img src="'+i+'" title="" alt="" data-align="center"><p>受到ChatGPT的启发，计算机视觉和视觉语言社区的研究人员很自然地会提出这样的问题:在视觉、视觉语言和多模态模型方面，ChatGPT/GPT-4的对应产品是什么？像ChatGPT这样的语言构建通用助手的路线图是明确的，同样探索可行的解决方案来构建计算机视觉的对应物——通用视觉助手正变得越来越重要。</p><p>本文将提到的多模态基础模型限制在<strong>视觉和视觉-语言模型</strong>领域。最近相关的论文有包括：（1）图像理解模型，如自监督学习【...】，SAM；（2）图像生成模型【...】；（3）视觉-语言预训练（VLP）。现有的VLP综述论文涵盖了在预训练、图像文本任务、核心视觉任务和/或视频文本任务时代之前针对任务特定VL问题的VLP方法【...】，最近的两篇综述论文涵盖了视觉模型与LLM的集成【...】。</p><h2 id="什么是多模态基础模型" tabindex="-1"><a class="header-anchor" href="#什么是多模态基础模型" aria-hidden="true">#</a> 什么是多模态基础模型？</h2><p>随着模型的兴起，人工智能正在经历一场范式转变随着模型的兴起，人工智能正在经历一场范式转变</p>',7);function _(h,u){const o=n("ExternalLinkIcon");return r(),s("div",null,[e("blockquote",null,[e("p",null,[t("从本文开始记录文章 "),e("a",d,[t("Multimodal Foundation Models: From Specialists to General-Purpose Assistants"),l(o)]),t(" 的阅读笔记，使用的是2023年9月的版本。文章引用的论文很多，在文中如果全部列出来十分麻烦，若文章仅仅是列出了论文的跳转索引，就用【...】来标记，不为每一篇论文附链接了。")])]),m])}const L=a(p,[["render",_],["__file","duomotaijichumoxingzongshu.html.vue"]]);export{L as default};
