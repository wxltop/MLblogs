import{_ as n,r as a,o as s,c as i,a as e,b as r,d as o}from"./app-aa9cafec.js";const l={},c=e("div",{class:"custom-container info"},[e("svg",{xmlns:"http://www.w3.org/2000/svg","xmlns:xlink":"http://www.w3.org/1999/xlink",viewBox:"0 0 24 24"},[e("g",{fill:"none",stroke:"currentColor","stroke-width":"2","stroke-linecap":"round","stroke-linejoin":"round"},[e("circle",{cx:"12",cy:"12",r:"9"}),e("path",{d:"M12 8h.01"}),e("path",{d:"M11 12h1v4h1"})])]),e("p",{class:"custom-container-title"},"INFO"),e("p",null,"本专题记录MOT(多目标跟踪)任务中，用了transformer的论文。")],-1),h=e("h2",{id:"ticrossnet",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#ticrossnet","aria-hidden":"true"},"#"),r(" TicrossNet")],-1),d=e("p",null,"收录：2023",-1),_={href:"http://arxiv.org/abs/2307.05874",target:"_blank",rel:"noopener noreferrer"},p=e("p",null,"代码地址：未开源",-1),u=e("p",null,"论文基于FairMOT，将两帧图像物体的特征输入到一个transformer里，然后执行dual-softmax（该论文中称之为cross-softmax function），通常大部分算法都是使用匈牙利算法来解代价矩阵，这样执行dual-softmax之后，得到的就是",-1),f=e("h2",{id:"p3aformer",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#p3aformer","aria-hidden":"true"},"#"),r(" P3AFormer")],-1),b=e("p",null,"收录：ECCV 2022",-1),m={href:"http://arxiv.org/abs/2207.05518",target:"_blank",rel:"noopener noreferrer"},g={href:"https://github.com/dvlab-research/ECCV22-P3AFormer-Tracking-Objects-as-Pixel-wise-Distributions",target:"_blank",rel:"noopener noreferrer"},k=e("h2",{id:"stam",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#stam","aria-hidden":"true"},"#"),r(" STAM")],-1),x=e("p",null,"收录：2017",-1),M={href:"http://arxiv.org/abs/1708.02843",target:"_blank",rel:"noopener noreferrer"},T=e("p",null,"代码地址：未开源",-1),v=e("h2",{id:"smiletrack",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#smiletrack","aria-hidden":"true"},"#"),r(" SMILEtrack")],-1),w=e("p",null,"收录：2022",-1),O={href:"http://arxiv.org/abs/2211.08824",target:"_blank",rel:"noopener noreferrer"},C=e("p",null,"代码地址：未开源",-1),V=e("h2",{id:"strn",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#strn","aria-hidden":"true"},"#"),r(" STRN")],-1),S=e("p",null,"收录：2019",-1),j={href:"http://arxiv.org/abs/1904.11489",target:"_blank",rel:"noopener noreferrer"},E=e("p",null,"代码地址：未开源",-1),N=e("h2",{id:"s-vit",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#s-vit","aria-hidden":"true"},"#"),r(" S-ViT")],-1),P=e("p",null,"收录：CVPR 2023",-1),A={href:"http://arxiv.org/abs/2303.17228",target:"_blank",rel:"noopener noreferrer"},I={href:"https://github.com/yuzhms/Streaming-Video-Model",target:"_blank",rel:"noopener noreferrer"},y=e("h2",{id:"memot",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#memot","aria-hidden":"true"},"#"),r(" MeMOT")],-1),F=e("p",null,"收录：CVPR 2022",-1),L={href:"http://arxiv.org/abs/2203.16761",target:"_blank",rel:"noopener noreferrer"},R=e("p",null,"代码地址：未开源",-1);function B(D,z){const t=a("ExternalLinkIcon");return s(),i("div",null,[c,h,d,e("p",null,[r("论文地址："),e("a",_,[r("Multi-Object Tracking as Attention Mechanism"),o(t)])]),p,u,f,b,e("p",null,[r("论文地址："),e("a",m,[r("Tracking Objects as Pixel-wise Distributions"),o(t)])]),e("p",null,[r("代码地址："),e("a",g,[r("GitHub - dvlab-research/ECCV22-P3AFormer-Tracking-Objects-as-Pixel-wise-Distributions: The official code for our ECCV22 oral paper: tracking objects as pixel-wise distributions."),o(t)])]),k,x,e("p",null,[r("论文地址："),e("a",M,[r("Online Multi-Object Tracking Using CNN-based Single Object Tracker with Spatial-Temporal Attention Mechanism"),o(t)])]),T,v,w,e("p",null,[r("论文地址："),e("a",O,[r("SMILEtrack: SiMIlarity LEarning for Multiple Object Tracking"),o(t)])]),C,V,S,e("p",null,[r("论文地址："),e("a",j,[r("Spatial-Temporal Relation Networks for Multi-Object Tracking"),o(t)])]),E,N,P,e("p",null,[r("论文地址："),e("a",A,[r("Streaming Video Model"),o(t)])]),e("p",null,[r("代码地址："),e("a",I,[r('GitHub - yuzhms/Streaming-Video-Model: [CVPR2023] Code for "Streaming Video Model"'),o(t)])]),y,F,e("p",null,[r("论文地址："),e("a",L,[r(" MeMOT: Multi-Object Tracking with Memory"),o(t)])]),R])}const H=n(l,[["render",B],["__file","MOT2_transformer.html.vue"]]);export{H as default};
