import{_ as e,r as n,o as r,c as i,a as s,b as a,d as l}from"./app-aa9cafec.js";const m="/MLblogs/assets/2024-01-09-18-39-57-image-47212336.png",o="/MLblogs/assets/2024-01-09-16-41-43-image-6467de67.png",c="/MLblogs/assets/2024-01-09-17-18-45-image-2032cc37.png",p="/MLblogs/assets/2024-01-09-17-20-15-image-abb22194.png",h="/MLblogs/assets/2024-01-09-17-26-29-image-ca085f27.png",g="/MLblogs/assets/2024-01-09-17-27-02-image-0796e983.png",u={},d=s("div",{class:"custom-container info"},[s("svg",{xmlns:"http://www.w3.org/2000/svg","xmlns:xlink":"http://www.w3.org/1999/xlink",viewBox:"0 0 24 24"},[s("g",{fill:"none",stroke:"currentColor","stroke-width":"2","stroke-linecap":"round","stroke-linejoin":"round"},[s("circle",{cx:"12",cy:"12",r:"9"}),s("path",{d:"M12 8h.01"}),s("path",{d:"M11 12h1v4h1"})])]),s("p",{class:"custom-container-title"},"INFO"),s("p",null,"基于RGB图像的跟踪器已经非常强大，同样也有大量的相关数据集。然而基于纯RGB序列的目标跟踪在极端光照、背景杂波和运动模糊等复杂场景中仍然容易失败。因此，多模态跟踪越来越受到人们的关注，因为它能够利用多模态互补来实现更鲁棒的跟踪，如RGB+Depth (RGB- d)，RGB+Thermal (RGB-T), 和RGB+Event (RGBE)。之前的一些工作通常先在大型RGB数据中训练跟踪器，然后再在多模态数据集中微调模型。但是由于多模态数据集相对较小，模型往往容易忘记在RGB数据中所学习到的知识，而且容易过拟合，随着NLP中的prompt技术的提出，越来越多的方法使用基于视觉信息的prompt来微调视觉模型。在多模态跟踪中也慢慢出现一些相关基于prompt的方法。这类方法甚至可以灵活切换到不同的模态组合，达到更优的结果的同时，需要训练的参数量也更少。")],-1),_=s("h2",{id:"jmmac",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#jmmac","aria-hidden":"true"},"#"),a(" JMMAC")],-1),b={href:"https://arxiv.org/abs/2007.02041",target:"_blank",rel:"noopener noreferrer"},y={href:"https://github.com/zhang-pengyu/JMMAC",target:"_blank",rel:"noopener noreferrer"},f=s("h2",{id:"cmpp",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#cmpp","aria-hidden":"true"},"#"),a(" CMPP")],-1),v={href:"https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Cross-Modal_Pattern-Propagation_for_RGB-T_Tracking_CVPR_2020_paper.pdf",target:"_blank",rel:"noopener noreferrer"},k=s("p",null,"代码：未开源",-1),x=s("h2",{id:"apfnet",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#apfnet","aria-hidden":"true"},"#"),a(" APFNet")],-1),M={href:"https://github.com/yangmengmeng1997/APFNet/blob/main/Paper/Attribute_based_Progressive_Fusion_Network_for_RGBT_Tracking.pdf",target:"_blank",rel:"noopener noreferrer"},B={href:"https://github.com/yangmengmeng1997/APFNet",target:"_blank",rel:"noopener noreferrer"},R=s("h2",{id:"spt",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#spt","aria-hidden":"true"},"#"),a(" SPT")],-1),w={href:"https://arxiv.org/abs/2208.09787",target:"_blank",rel:"noopener noreferrer"},G={href:"https://github.com/xuefeng-zhu5/RGBD1K",target:"_blank",rel:"noopener noreferrer"},T=s("p",null,"文章提出来RGB1K数据集，也提出来一个baseline，但是GitHub没有baseline的代码，其结构如下：",-1),z=s("img",{src:m,title:"",alt:"","data-align":"center"},null,-1),P=s("h2",{id:"protrack",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#protrack","aria-hidden":"true"},"#"),a(" ProTrack")],-1),V={href:"http://arxiv.org/abs/2207.14571",target:"_blank",rel:"noopener noreferrer"},C=s("p",null,"代码：未开源",-1),A=s("p",null,"简要介绍：通过将输入的多个模态（两个模态）融合为单个模态，冻结在RGB数据集中预训练的模型，利用该模型来处理融合后的输入，得到预测结果。文章没有训练过程。本文也是第一篇将prompt learning基于用在多模态跟踪上的论文。",-1),L=s("img",{src:o,title:"",alt:"","data-align":"center"},null,-1),D=s("p",null,"文章核心是下面的公式：",-1),N=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",null,"f"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"V"),s("mi",null,"t")]),s("mo",{separator:"true"},","),s("msub",null,[s("mi",null,"A"),s("mi",null,"t")]),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mi",null,"λ"),s("mo",null,"∗"),s("mi",null,"C"),s("mi",null,"o"),s("mi",null,"l"),s("mi",null,"o"),s("mi",null,"r"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"A"),s("mi",null,"t")]),s("mo",{stretchy:"false"},")"),s("mo",null,"+"),s("mo",{stretchy:"false"},"("),s("mn",null,"1"),s("mo",null,"−"),s("mi",null,"λ"),s("mo",{stretchy:"false"},")"),s("mo",null,"∗"),s("mi",null,"C"),s("mi",null,"o"),s("mi",null,"l"),s("mi",null,"o"),s("mi",null,"r"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"V"),s("mi",null,"t")]),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"}," f(V_t, A_t)=\\lambda*Color(A_t)+(1-\\lambda)*Color(V_t) ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.10764em"}},"f"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.22222em"}},"V"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.2222em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"A"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6944em"}}),s("span",{class:"mord mathnormal"},"λ"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"∗"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"C"),s("span",{class:"mord mathnormal"},"o"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"or"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"A"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"("),s("span",{class:"mord"},"1"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"−"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"λ"),s("span",{class:"mclose"},")"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"∗"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"C"),s("span",{class:"mord mathnormal"},"o"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"or"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.22222em"}},"V"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.2222em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mclose"},")")])])])])],-1),j=s("p",null,[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"V"),s("mi",null,"t")])]),s("annotation",{encoding:"application/x-tex"},"V_t")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.22222em"}},"V"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"-0.2222em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("表示RGB图像，"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msub",null,[s("mi",null,"A"),s("mi",null,"t")])]),s("annotation",{encoding:"application/x-tex"},"A_t")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8333em","vertical-align":"-0.15em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"A"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2806em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"t")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])])])])]),a("是额外的模态输入，如深度信息。使用一个"),s("strong",null,"染色函数"),a(" Color来处理输入的图像，若是RGB图像，不做处理；若只有单通道，则该函数将单通道转为多通道。具体而言，对于深度图像和热图像，使用Color函数求其JET Colormaps，对于event图像，我使用从事件流转换过来的事件图像。"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"λ"),s("mo",null,"="),s("mn",null,"0.05")]),s("annotation",{encoding:"application/x-tex"},"\\lambda=0.05")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6944em"}}),s("span",{class:"mord mathnormal"},"λ"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6444em"}}),s("span",{class:"mord"},"0.05")])])]),a("。")],-1),E=s("h2",{id:"vipt",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#vipt","aria-hidden":"true"},"#"),a(" ViPT")],-1),F={href:"http://arxiv.org/abs/2303.10826",target:"_blank",rel:"noopener noreferrer"},J={href:"https://github.com/jiawen-zhu/ViPT",target:"_blank",rel:"noopener noreferrer"},K=s("p",null,"简要介绍：冻结在RGB图像中预训练的ViT部分，引入少量的prompter用于模态的互相融合，在下游的多模态数据集上微调。将额外模态数据作为prompts，RGB图像作为主模态。",-1),O=s("img",{src:c,title:"",alt:"","data-align":"center"},null,-1),I=s("p",null,"更详细点的结构如下：",-1),H=s("p",null,"首先将两种模态的原始数据转为tokens，然后在ViT中每一层Transformer中引入可学习的prompter，用于将两个模态的tokens互相融合。其中所有关于RGB图像的网络部分全部冻结，只训练额外模态部分的网络。",-1),S=s("img",{src:p,title:"",alt:"","data-align":"center"},null,-1),W=s("p",null,"文中使用OSTrack作为RGB的基础模型。prompter设计也很简单，先将两种tokens降维，然后融合，再升维：",-1),q=s("img",{src:h,title:"",alt:"","data-align":"center"},null,-1),Q=s("p",null,"文中对于ViPT的核心结构也做了消融实验，发现最后一种结果最好：",-1),U=s("img",{src:g,title:"",alt:"","data-align":"center"},null,-1);function X(Y,Z){const t=n("ExternalLinkIcon");return r(),i("div",null,[d,_,s("p",null,[a("论文："),s("a",b,[a("[2007.02041] Jointly Modeling Motion and Appearance Cues for Robust RGB-T Tracking (arxiv.org)"),l(t)])]),s("p",null,[a("代码："),s("a",y,[a("zhang-pengyu/JMMAC: The official repository of 'Jointly Modeling Motion and Appearance Cues for Robust RGB-T Tracking'. (github.com)"),l(t)])]),f,s("p",null,[a("论文："),s("a",v,[a("Cross-modal patternpropagation for RGB-T tracking"),l(t)])]),k,x,s("p",null,[a("论文："),s("a",M,[a("paper"),l(t)])]),s("p",null,[a("代码："),s("a",B,[a("yangmengmeng1997/APFNet (github.com)"),l(t)])]),R,s("p",null,[a("论文："),s("a",w,[a("[2208.09787] RGBD1K: A Large-scale Dataset and Benchmark for RGB-D Object Tracking (arxiv.org)"),l(t)])]),s("p",null,[a("GitHub："),s("a",G,[a('xuefeng-zhu5/RGBD1K: Dataset for the paper "RGBD1K: A Large-scale Dataset and Benchmark for RGB-D Object Tracking". (github.com)'),l(t)])]),T,z,P,s("p",null,[a("论文："),s("a",V,[a("[2207.14571] Prompting for Multi-Modal Tracking"),l(t)])]),C,A,L,D,N,j,E,s("p",null,[a("论文："),s("a",F,[a("[2303.10826] Visual Prompt Multi-Modal Tracking"),l(t)])]),s("p",null,[a("代码："),s("a",J,[a("jiawen-zhu/ViPT: [CVPR23] Visual Prompt Multi-Modal Tracking (github.com)"),l(t)])]),K,O,I,H,S,W,q,Q,U])}const ss=e(u,[["render",X],["__file","multi_modal_mot.html.vue"]]);export{ss as default};
